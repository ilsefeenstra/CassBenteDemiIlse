{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Final model\nIn this script we will run our final model. We will run a k-fold to obtain a reliable result of our performance. We will also run a seperate model on all the train data in order to get a model we can test with the test data.","metadata":{}},{"cell_type":"code","source":"import zipfile\nimport os\nimport cv2\nimport pandas as pd\nfrom os import chdir, listdir\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, preprocessing, regularizers\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.losses import MeanSquaredError\nfrom keras import backend as K\nfrom keras import activations","metadata":{"id":"vRjBTUzjlmKA","execution":{"iopub.status.busy":"2022-02-03T10:49:56.974065Z","iopub.execute_input":"2022-02-03T10:49:56.974753Z","iopub.status.idle":"2022-02-03T10:50:02.416275Z","shell.execute_reply.started":"2022-02-03T10:49:56.974652Z","shell.execute_reply":"2022-02-03T10:50:02.415512Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Import tabular data\n\nThe tabular data is imported. This contains information on whether several elements are present in the image, such as blur, a human, a group, etc. Also the pawpularity score of the training data is in the table. For the test data only the image ID and the features are in the table. There is also a sample submission table, which contains the pawpularity score for the test data.","metadata":{"id":"0ae29a8e"}},{"cell_type":"code","source":"# load train, test, and submission sample dataset.\ncsv_train_data = pd.read_csv('/kaggle/input/petfinder-pawpularity-score/train.csv')\ncsv_test_data = pd.read_csv('/kaggle/input/petfinder-pawpularity-score/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/petfinder-pawpularity-score/sample_submission.csv')\ncsv_train_data.head()\n\n# Drop rows with missing values (if NaN values are in dataframe)\n# No missing values present, so no samples dropped\ncsv_train_data.dropna()","metadata":{"id":"8ae10a3a","outputId":"a7c00d1f-60f9-472f-d66b-1d270ef6948b","execution":{"iopub.status.busy":"2022-02-03T10:50:02.423076Z","iopub.execute_input":"2022-02-03T10:50:02.424401Z","iopub.status.idle":"2022-02-03T10:50:02.505896Z","shell.execute_reply.started":"2022-02-03T10:50:02.424361Z","shell.execute_reply":"2022-02-03T10:50:02.505236Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Create a plot that shows the distribution of the output of the training samples\nplt.hist(csv_train_data['Pawpularity'], bins=100)\nplt.title(\"Data distribution of the tabular data\")\nplt.xlabel(\"Pawpularity score\")\nplt.ylabel(\"Occurence\")\nplt.xlim(0, 100)\n\nplt.show()","metadata":{"id":"SRYM1P29o8k1","outputId":"1bfe2428-76b6-477a-b608-0232a40d37f5","execution":{"iopub.status.busy":"2022-02-03T10:50:02.507241Z","iopub.execute_input":"2022-02-03T10:50:02.507526Z","iopub.status.idle":"2022-02-03T10:50:02.848675Z","shell.execute_reply.started":"2022-02-03T10:50:02.507480Z","shell.execute_reply":"2022-02-03T10:50:02.847876Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Import image data\nThe images are imported from the folders. Each image is reshaped to a 64x64 image. In this way all the images have the same shape and we do not use much memory, to speed up analysis. After the images are imported, the images and their names are shuffled. This is done, so we can later take a validation sample containing a random subsample of the dataset. It could be that the images in the dataset contain some order, so by shuffling we ensure that the subset for the validation data is random.\n","metadata":{"id":"5eccde6f"}},{"cell_type":"code","source":"def reshape_images(path, n):\n    \"\"\"\n    This function returns a list of images, which are reshaped to 64 x 64 \n    and a list with the names of the images.\n    \"\"\"\n    # Set the current path\n    chdir(path)\n    \n    # Preset the lists\n    images = []\n    image_names = []\n    \n    # Go over all the files in the path\n    for i in listdir():\n        \n        # Get the name of the image, without .jpg\n        image_names.append(i[:-4])\n        \n        # Get the image and reshape to n x n\n        file = cv2.imread(i)\n        file = cv2.resize(file,(n, n), interpolation=cv2.INTER_AREA)\n        \n        # Rescale the pixels and store in the list\n        images.append(file/255)\n        \n    return images, image_names\n\n# Reshape train and test images\n# Set the path for loading image dataset.\ntrain_imgs, train_names = reshape_images('/kaggle/input/petfinder-pawpularity-score/train', 64)\ntest_imgs, test_names = reshape_images('/kaggle/input/petfinder-pawpularity-score/test', 64)","metadata":{"id":"6a8970b3","execution":{"iopub.status.busy":"2022-02-03T10:50:02.851190Z","iopub.execute_input":"2022-02-03T10:50:02.851598Z","iopub.status.idle":"2022-02-03T10:53:06.686598Z","shell.execute_reply.started":"2022-02-03T10:50:02.851558Z","shell.execute_reply":"2022-02-03T10:53:06.685833Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Combine tabular data with images\nTo ensure that the dataframe has the same order as the images in the list, we sort the dataframe based on the names of the images. If this would not be the case, it could be that you learn incorrectly, as the output of an image perhaps is not the real output.","metadata":{"id":"8b818ae9"}},{"cell_type":"code","source":"def sort_dataframe(data, images, names):\n    \"\"\"\n    This function sorts the dataframe of the csv data according to the image names.\n    \"\"\"\n    data_sorted = pd.DataFrame()\n\n    # Iterate over images and get index of each image\n    for img, name in zip(images, names):\n        location = data[data['Id'] == name].index[0]\n\n        # Sort dataframe according to index of images\n        data_sorted = data_sorted.append([data.loc[location]])\n\n        # Reset the index of the dataframe\n        data_sorted = data_sorted.reset_index().drop(['index'],axis=1)\n        \n    return data_sorted\n\n# Sort training and testing data\ntrain_data_sorted = sort_dataframe(csv_train_data, train_imgs, train_names)\ntest_data_sorted = sort_dataframe(csv_test_data, test_imgs, test_names)\nsample_submission_sorted = sort_dataframe(sample_submission, test_imgs, test_names)","metadata":{"id":"85185f99","execution":{"iopub.status.busy":"2022-02-03T10:53:06.687951Z","iopub.execute_input":"2022-02-03T10:53:06.688205Z","iopub.status.idle":"2022-02-03T10:53:56.031240Z","shell.execute_reply.started":"2022-02-03T10:53:06.688172Z","shell.execute_reply":"2022-02-03T10:53:56.030512Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Processing data\nThe tabular data is split in x and y values and converted to numpy arrays, so the neural network can handle the data. Moreover, the image data is converted to numpy arrays.","metadata":{"id":"e9206b4b"}},{"cell_type":"code","source":"# Remove samples with pawpularity score of 100\nindexNames = train_data_sorted[train_data_sorted['Pawpularity'] == 100].index | train_data_sorted[train_data_sorted['Pawpularity'] < 5].index\ntrain_data_new = train_data_sorted.drop(indexNames)\ntrain_imgs_new = np.delete(train_imgs, indexNames, axis=0)","metadata":{"id":"CxecT9dhMfbF","execution":{"iopub.status.busy":"2022-02-03T10:53:56.032575Z","iopub.execute_input":"2022-02-03T10:53:56.032810Z","iopub.status.idle":"2022-02-03T10:53:56.037992Z","shell.execute_reply.started":"2022-02-03T10:53:56.032778Z","shell.execute_reply":"2022-02-03T10:53:56.036292Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Select x-values (the 12 input features) and y-values from training data\nx_tabular = train_data_new.iloc[:,1:13].to_numpy()\ny = train_data_new.iloc[:,13].to_numpy()\n\n# Select x (the 12 input features) and y (pawpularity) values from testing data\nx_test_tabular = test_data_sorted.iloc[:,1:13].to_numpy()\ny_test = sample_submission_sorted.iloc[:,1].to_numpy()\n\n# Create numpy array of image data \nx_images = np.array(train_imgs_new)\ntest_imgs_array = np.array(test_imgs)","metadata":{"id":"Agl1YxFwNcKo","execution":{"iopub.status.busy":"2022-02-03T10:53:56.039566Z","iopub.execute_input":"2022-02-03T10:53:56.039829Z","iopub.status.idle":"2022-02-03T10:53:56.345648Z","shell.execute_reply.started":"2022-02-03T10:53:56.039794Z","shell.execute_reply":"2022-02-03T10:53:56.344844Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Create seperate neural networks\nWe create a tabular neural network to handle the data in the csv. Then we create a convolutional neural network to handle the image data. Both neural networks have no output layer, since they will be concatenated to one neural network, which will give the output.","metadata":{"id":"RNE8NWL7xgmp"}},{"cell_type":"code","source":"def build_neural_net(input_size, hidden_nodes):\n    \"\"\"\n    Build neural network with an input size and a hidden layer with a number of \n    hidden nodes.\n    \"\"\"\n    # Create a sequential model object\n    model = models.Sequential()\n\n    # Create hidden layer \n    model.add(layers.Dense(units=hidden_nodes, activation='relu', input_shape=(input_size,)))    \n\n    # Create hidden layer \n    model.add(layers.Dropout(0.4))\n    model.add(layers.Dense(units=hidden_nodes, activation=\"relu\"))\n\n    # Create hidden layer \n    model.add(layers.Dropout(0.4))\n    model.add(layers.Dense(units=hidden_nodes, activation=\"relu\"))\n\n    return model","metadata":{"id":"ccf1cb99","execution":{"iopub.status.busy":"2022-02-03T10:53:56.347078Z","iopub.execute_input":"2022-02-03T10:53:56.347348Z","iopub.status.idle":"2022-02-03T10:53:56.354356Z","shell.execute_reply.started":"2022-02-03T10:53:56.347311Z","shell.execute_reply":"2022-02-03T10:53:56.353300Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def build_convol_net(image_size, hidden_nodes):\n    \"\"\"\n    Build neural network with an input size and a hidden layer with a number \n    of hidden nodes.\n    \"\"\"\n    # Create a sequential model object\n    model = models.Sequential()\n    \n    # Create a convolutional layer \n    model.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', input_shape=image_size, padding='same'))\n    model.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', input_shape=image_size, padding='same'))\n    model.add(layers.MaxPool2D(pool_size=(2,2), strides=2))\n    model.add(layers.BatchNormalization())\n\n    # Create a convolutional layer \n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same'))\n    model.add(layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same'))\n    model.add(layers.MaxPool2D(pool_size=(2,2), strides=2))\n    model.add(layers.BatchNormalization())\n\n    # Create a convolutional layer \n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same'))\n    model.add(layers.Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same'))\n    model.add(layers.MaxPool2D(pool_size=(2,2), strides=2))\n    model.add(layers.BatchNormalization())\n    \n    # Create a flattening layer\n    model.add(layers.Flatten())\n\n    # Create a dense layer \n    model.add(layers.Dropout(0.3))\n    model.add(layers.Dense(units=hidden_nodes, activation=\"relu\", \n              kernel_regularizer=regularizers.l2(1e-3),\n              bias_regularizer=regularizers.l2(1e-3),\n              activity_regularizer=regularizers.l2(1e-3)))\n\n    # Create a dense layer \n    model.add(layers.Dropout(0.3))\n    model.add(layers.Dense(units=hidden_nodes, activation=\"relu\", \n              kernel_regularizer=regularizers.l2(1e-3),\n              bias_regularizer=regularizers.l2(1e-3),\n              activity_regularizer=regularizers.l2(1e-3)))\n    \n    return model","metadata":{"id":"bd18b2f9","execution":{"iopub.status.busy":"2022-02-03T10:53:56.356265Z","iopub.execute_input":"2022-02-03T10:53:56.356870Z","iopub.status.idle":"2022-02-03T10:53:56.371880Z","shell.execute_reply.started":"2022-02-03T10:53:56.356833Z","shell.execute_reply":"2022-02-03T10:53:56.370955Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Concatenate tabular and image data models\nConcatenate the tabular and image models to create one neural network that can handle both types of data. This neural network will give the prediction of the pawpularity.","metadata":{"id":"vtZpIRuAue4B"}},{"cell_type":"code","source":"def linear_limit(x):\n    \"\"\"\n    Create a linear activation function that clips the output at 0 and 100.\n    \"\"\"\n    activation_x = activations.linear(x)\n    activation_x_new = K.clip(activation_x, 0, 100)\n\n    return activation_x_new","metadata":{"id":"ZKkweNdcNXLl","execution":{"iopub.status.busy":"2022-02-03T10:53:56.373484Z","iopub.execute_input":"2022-02-03T10:53:56.373787Z","iopub.status.idle":"2022-02-03T10:53:56.383833Z","shell.execute_reply.started":"2022-02-03T10:53:56.373729Z","shell.execute_reply":"2022-02-03T10:53:56.383067Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def concatenate_models(model1, model2, hidden_nodes):\n    \"\"\"\n    Concatenate two neural network models, model1 and model2, and create\n    a concatenated model with dense layers with some hidden nodes.\n    \"\"\"\n    # Input for concatenated model is retrieved by concatenating the output\n    # of both models\n    concat_input = layers.concatenate([model1.output, model2.output])\n\n    # Create hidden layer \n    hidden_layer_1 = layers.Dense(hidden_nodes, activation=\"relu\", \n              kernel_regularizer=regularizers.l2(1e-1),\n              bias_regularizer=regularizers.l2(1e-1),\n              activity_regularizer=regularizers.l2(1e-1))(concat_input)\n\n    # Create hidden layer \n    drop_out_1 = layers.Dropout(0.4)(hidden_layer_1)    \n    hidden_layer_2 = layers.Dense(hidden_nodes, activation=\"relu\", \n              kernel_regularizer=regularizers.l2(1e-1),\n              bias_regularizer=regularizers.l2(1e-1),\n              activity_regularizer=regularizers.l2(1e-1))(drop_out_1)\n\n    # Create hidden layer \n    drop_out_2 = layers.Dropout(0.4)(hidden_layer_2)\n    hidden_layer_3 = layers.Dense(hidden_nodes, activation=\"relu\", \n              kernel_regularizer=regularizers.l2(1e-1),\n              bias_regularizer=regularizers.l2(1e-1),\n              activity_regularizer=regularizers.l2(1e-1))(drop_out_2)\n\n    # Create output layer\n    output_layer = layers.Dense(1, activation=linear_limit)(hidden_layer_3)\n\n    # Create concatenated model with inputs of both models and output of the\n    # concatenated model\n    concat_model = models.Model(inputs=[model1.input, model2.input], outputs=output_layer)\n\n    return concat_model","metadata":{"id":"1441c455","execution":{"iopub.status.busy":"2022-02-03T10:53:56.385329Z","iopub.execute_input":"2022-02-03T10:53:56.385997Z","iopub.status.idle":"2022-02-03T10:53:56.397886Z","shell.execute_reply.started":"2022-02-03T10:53:56.385949Z","shell.execute_reply":"2022-02-03T10:53:56.397135Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Part of code from: https://www.tensorflow.org/tutorials/keras/regression\n\ndef plot_loss(history):\n    \"\"\"\n    Plot loss during epochs of training a neural network.\n    \"\"\"\n    \n    fig, axs = plt.subplots(1,2,figsize=(20,5)) \n\n    for i, metric in enumerate(['loss', 'root_mean_squared_error']):\n        axs[i].plot(history.history[metric])\n        axs[i].legend(['training'], loc='best')\n\n        axs[i].set_title('Model '+metric)\n        axs[i].set_ylabel(metric)\n        axs[i].set_xlabel('epoch')\n\n    plt.show()\n\n\ndef train_and_evaluate(model, image_x, tabular_x, train_y, \n                       x_test_imgs, x_test_tabular, test_y, epochs=20, preprocess = {}, augment={}):\n  \"\"\"\n  This function trains and evaluated a model. It first compiles the model with \n  the loss and metrics. It then makes a train and validation generator for the \n  image data, based on the preprocess and augment input. \n  It then trains the model on both the image and tabular data for epochs times. \n  The values of the loss and metric are plotted and printed.\n  \"\"\"\n\n  # Compile model and use mean squared error as loss and root mean squared error as metric\n  model.compile(loss=MeanSquaredError(), metrics=[RootMeanSquaredError()])\n\n  # Preprocess the image data\n  train_gen = preprocessing.image.ImageDataGenerator(**preprocess, **augment)\n  train_gen.fit(image_x)\n\n  val_gen = preprocessing.image.ImageDataGenerator(**preprocess)\n  val_gen.fit(image_x)\n\n  # Train the model by fitting both tabular and image data at the same time\n  history = model.fit(train_gen.flow([image_x, tabular_x], train_y), epochs=epochs)\n\n  # Plot the loss and metric\n  plot_loss(history)\n\n  # Evaluate the model on the test data\n  test_accuracy = model.evaluate(val_gen.flow([x_test_imgs, x_test_tabular], test_y))\n\n  return test_accuracy\n\n# Make Neural Networks before concatenation\ntabular_NN = build_neural_net(12, hidden_nodes=20)\nimage_size = (64, 64, 3)\nimage_NN = build_convol_net(image_size, hidden_nodes=20)\n\n# Concatenate tabular and image neural networks\nconcat_model = concatenate_models(image_NN, tabular_NN, hidden_nodes=20)\n\n# Train model on both tabular and image data and preprocess\ntest_acc = train_and_evaluate(concat_model, x_images, x_tabular, y, test_imgs_array, x_test_tabular, y_test,\n                   preprocess={'featurewise_center': True, 'featurewise_std_normalization': True}, epochs=60)\n\nprint(f\"Test MSE: {test_acc[0]}, Test RMSE: {test_acc[1]}\")","metadata":{"id":"UiMunH5vM8wU","execution":{"iopub.status.busy":"2022-02-03T10:53:56.399359Z","iopub.execute_input":"2022-02-03T10:53:56.400462Z","iopub.status.idle":"2022-02-03T11:00:46.157893Z","shell.execute_reply.started":"2022-02-03T10:53:56.400422Z","shell.execute_reply":"2022-02-03T11:00:46.157187Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Predict results for the test data set\nimg_result = concat_model.predict([test_imgs_array, x_test_tabular])\nfinal_result = pd.DataFrame(img_result)\nfinal_result.columns =['Pawpularity']","metadata":{"execution":{"iopub.status.busy":"2022-02-03T11:00:46.160604Z","iopub.execute_input":"2022-02-03T11:00:46.160859Z","iopub.status.idle":"2022-02-03T11:00:46.412616Z","shell.execute_reply.started":"2022-02-03T11:00:46.160826Z","shell.execute_reply":"2022-02-03T11:00:46.411956Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Store predictions of the Pawpularity in a csv file\nfor ids, paw in zip(test_data_sorted['Id'], final_result['Pawpularity']):\n    location = sample_submission[sample_submission['Id'] == ids].index[0]\n    sample_submission['Pawpularity'].loc[location] = paw \nsample_submission.to_csv('/kaggle/working/submission.csv',index=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-03T11:00:46.413829Z","iopub.execute_input":"2022-02-03T11:00:46.414105Z","iopub.status.idle":"2022-02-03T11:00:46.430941Z","shell.execute_reply.started":"2022-02-03T11:00:46.414068Z","shell.execute_reply":"2022-02-03T11:00:46.430110Z"},"trusted":true},"execution_count":15,"outputs":[]}]}