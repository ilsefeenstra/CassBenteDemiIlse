{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3915ca73",
      "metadata": {
        "id": "3915ca73"
      },
      "source": [
        "# Linear output\n",
        "In this chapter we will limit the linear output between 0 and 100, so the model will be more realistic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "vRjBTUzjlmKA",
      "metadata": {
        "id": "vRjBTUzjlmKA"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from os import chdir, listdir\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, preprocessing, regularizers\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from keras import backend as K\n",
        "from keras import activations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LFZWcvRBvYv_",
      "metadata": {
        "id": "LFZWcvRBvYv_"
      },
      "source": [
        "# Import zip with the data\n",
        "The data is imported as a zip from the github of our project group. The zip is unpacked in the google colab, so the data is accesible. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fANqjfPxoHI7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fANqjfPxoHI7",
        "outputId": "8c4a2339-2ed3-45fc-c245-6d428d72c3a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-25 13:52:41--  https://github.com/ilsefeenstra/Fitgirls0011/archive/refs/heads/main.zip\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/ilsefeenstra/Fitgirls0011/zip/refs/heads/main [following]\n",
            "--2022-01-25 13:52:41--  https://codeload.github.com/ilsefeenstra/Fitgirls0011/zip/refs/heads/main\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.112.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.112.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘/tmp/pawpularity_data.zip’\n",
            "\n",
            "/tmp/pawpularity_da     [         <=>        ] 993.21M  21.1MB/s    in 47s     \n",
            "\n",
            "2022-01-25 13:53:29 (21.1 MB/s) - ‘/tmp/pawpularity_data.zip’ saved [1041452105]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Code from: https://towardsdatascience.com/an-informative-colab-guide-to-load-image-datasets-from-github-kaggle-and-local-machine-75cae89ffa1e\n",
        "\n",
        "# Get zip file from Github URL\n",
        "!wget --no-check-certificate \\\n",
        "    \"https://github.com/ilsefeenstra/Fitgirls0011/archive/refs/heads/main.zip\" \\\n",
        "    -O \"/tmp/pawpularity_data.zip\"\n",
        "\n",
        "# Opens the zip file in read mode and extract files into /tmp folder\n",
        "zip_ref = zipfile.ZipFile('/tmp/pawpularity_data.zip', 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ae29a8e",
      "metadata": {
        "id": "0ae29a8e"
      },
      "source": [
        "# Import tabular data\n",
        "\n",
        "The tabular data is imported. This contains information on whether several elements are present in the image, such as blur, a human, a group, etc. Also the pawpularity score of the training data is in the table. For the test data only the image ID and the features are in the table. There is also a sample submission table, which contains the pawpularity score for the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8ae10a3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "8ae10a3a",
        "outputId": "ee62acfa-5cf6-4c88-ade5-3b43713083cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7b201466-48f7-4e6b-8a6d-96b079a1a016\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Subject Focus</th>\n",
              "      <th>Eyes</th>\n",
              "      <th>Face</th>\n",
              "      <th>Near</th>\n",
              "      <th>Action</th>\n",
              "      <th>Accessory</th>\n",
              "      <th>Group</th>\n",
              "      <th>Collage</th>\n",
              "      <th>Human</th>\n",
              "      <th>Occlusion</th>\n",
              "      <th>Info</th>\n",
              "      <th>Blur</th>\n",
              "      <th>Pawpularity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0007de18844b0dbbb5e1f607da0606e0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0009c66b9439883ba2750fb825e1d7db</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0013fd999caf9a3efe1352ca1b0d937e</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0018df346ac9c1d8413cfcc888ca8246</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>001dc955e10590d3ca4673f034feeef2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9907</th>\n",
              "      <td>ffbfa0383c34dc513c95560d6e1fdb57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9908</th>\n",
              "      <td>ffcc8532d76436fc79e50eb2e5238e45</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9909</th>\n",
              "      <td>ffdf2e8673a1da6fb80342fa3b119a20</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9910</th>\n",
              "      <td>fff19e2ce11718548fa1c5d039a5192a</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9911</th>\n",
              "      <td>fff8e47c766799c9e12f3cb3d66ad228</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9912 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b201466-48f7-4e6b-8a6d-96b079a1a016')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7b201466-48f7-4e6b-8a6d-96b079a1a016 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7b201466-48f7-4e6b-8a6d-96b079a1a016');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                    Id  Subject Focus  ...  Blur  Pawpularity\n",
              "0     0007de18844b0dbbb5e1f607da0606e0              0  ...     0           63\n",
              "1     0009c66b9439883ba2750fb825e1d7db              0  ...     0           42\n",
              "2     0013fd999caf9a3efe1352ca1b0d937e              0  ...     0           28\n",
              "3     0018df346ac9c1d8413cfcc888ca8246              0  ...     0           15\n",
              "4     001dc955e10590d3ca4673f034feeef2              0  ...     0           72\n",
              "...                                ...            ...  ...   ...          ...\n",
              "9907  ffbfa0383c34dc513c95560d6e1fdb57              0  ...     1           15\n",
              "9908  ffcc8532d76436fc79e50eb2e5238e45              0  ...     0           70\n",
              "9909  ffdf2e8673a1da6fb80342fa3b119a20              0  ...     0           20\n",
              "9910  fff19e2ce11718548fa1c5d039a5192a              0  ...     0           20\n",
              "9911  fff8e47c766799c9e12f3cb3d66ad228              0  ...     0           30\n",
              "\n",
              "[9912 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Import the CSV tables\n",
        "csv_train_data = pd.read_csv(\"/tmp/Fitgirls0011-main/petfinder-pawpularity-score/train.csv\")\n",
        "csv_test_data = pd.read_csv(\"/tmp/Fitgirls0011-main/petfinder-pawpularity-score/test.csv\")\n",
        "sample_submission = pd.read_csv(\"/tmp/Fitgirls0011-main/petfinder-pawpularity-score/sample_submission.csv\")\n",
        "csv_train_data.head()\n",
        "\n",
        "# Drop rows with missing values (if NaN values are in dataframe)\n",
        "# No missing values present, so no samples dropped\n",
        "csv_train_data.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "SRYM1P29o8k1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "SRYM1P29o8k1",
        "outputId": "7b11fc01-0a7d-41f5-86f4-3be60f2cc2b4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfyElEQVR4nO3deZwcVb338c+XsEskLDFCEgxqEMEFMBJQLw/C1Ue2C3qRRZFFNBcNj6CgIG7RK8p9LoIiiEZBQJFVkIgosuhFFNAEEAjBlxGDSQgkBAJJWBN+949z2lSanpmqyfR09/T3/XrNa6pObb+uqalfn1NVpxQRmJmZlbVWqwMwM7PO4sRhZmaVOHGYmVklThxmZlaJE4eZmVXixGFmZpU4cVi/SfqtpI/k4Q9K+vUArnumpN3z8BRJPx7AdZ8i6QcDtb4K232vpLmSlknascT8u0uaNxixNdj2OEkhae1+Lj9H0r8OdFyDtX7rnRNHh8j/KM9IWippiaQ/SDpGUqm/4ZqeCPoSERdHxLtLxHGBpK+WWN/2EfHbNY2r0ck3Ir4WER9Z03X3w+nAsRGxUUTcVT8x/31e24wNSzpS0q3NWHe7a+Z+7VZOHJ1lv4gYDrwKOA04CTivtSENrGYltjbxKmBmq4NoZ0P87z9kOHF0oIh4MiKmAQcDR0h6A4CkfSTdJemp3CQypbDYLfn3ktxUsquk10i6WdJiSY9JuljSiJ62K+ldkh6Q9KSkswEVpv3zG62SMyUtzLHcK+kNkiYBHwQ+k2P4eZ5/jqSTJN0DLJe0doOmiPUlXZZrXHdKenNh26t9o6zVaiS9DPglsGXe3jJJW9Y3fUn6t9w0tiQ3v72+MG2OpBMl3ZM/92WS1u9h/6wl6fOSHsqf/SJJG0taT9IyYBjwZ0l/a7Bs7e/z5xznwYVpJ+T1LZB0VKF8PUmnS/qHpEclfVfSBg3W/Xrgu8Cued1Lcnlvx0vNhyU9nLd9Yv0+Loz32KwmaWdJt+X9u0DS2ZLWLUwPSZMl/RX4aw/r+FDer4slfa7s+hvtV0mbSLpW0iJJT+ThMY22a405cXSwiPgjMA/4l1y0HDgcGAHsA3xM0gF52m7594jcVHIb6cT/dWBL4PXAWGBKo21J2hy4Cvg8sDnwN+DtPYT27ry9bYCNgYOAxRExFbgY+P85hv0KyxyaYx4RESsarHN/4ApgU+AnwM8krdPD9gGIiOXAXsDDeXsbRcTDdZ9rG+AS4HhgJHAd8PPiiS3H/x5ga+BNwJE9bPLI/PNO4NXARsDZEfFcRGyU53lzRLymQay7FaZvFBGX5fFXkvbhaOBo4BxJm+Rpp5H28Q7Aa/M8X2yw7lnAMcBted21Lwe9HS817wTGk/6mJ6l/1xVWAp8kHTe7AnsCH6+b5wBgIrBd/cKStgPOBT5EOlY3A4on+h7X38N+XQv4IakGuBXwDHB2Pz5X13Li6HwPk06mRMRvI+LeiHgxIu4hnRD/T08LRsTsiLghn9gWAWf0Mv/ewMyIuDIiXgC+CTzSw7wvAMOBbQFFxKyIWNDH5zgrIuZGxDM9TJ9R2PYZwPrALn2ss4yDgV/k/fAC6TrEBsDb6mJ7OCIeB35OOlE38kHgjIh4MCKWAZ8FDtGaNb+8AHwlIl6IiOuAZcDrJAmYBHwyIh6PiKXA14BDyq645PHy5YhYHhH3kk62h1b9ABExIyJuj4gVETEH+F6D7Xw9f45Gf/8DgWsj4paIeA74AvBixfUX41kcET+NiKfzfju1t/ntpdye2PlGA48DSJpI+hb6BmBdYD3St/SGJI0CvkWqsQwnfZF4oofZtwTm1kYiIiTNbTRjRNycm7LOAV4l6SrgxIh4qpfP0XBdjaZHxIu5WWTLPpYpY0vgobp1zyXt15pigny6l+2utq48vDYwCpjfz/gW19XAnibVZEYCGwIzUg4BUg1yWNkVlzxein+Xh4A3Vgk+b2cbUrKfkGNeG5jRy3bq1R97yyUtrrj+YjwbAmeSapG12ttwScMiYmXJj9XVXOPoYJLeSjrB1e6W+QkwDRgbERuT2rVrZ5VG3SB/LZe/MSJeDhxWmL/eAlJTVm3bKo7Xi4izIuItpKaHbYBP9xJHb+U1xW2vRWqqqDU7PU06YdS8ssJ6HyY1WdTWXftc/TnRr7YuUjPICuDRfqyrL4+Rmli2j4gR+WfjQpNYvUb7obfjpab4N96KVft8OT3v83rnAg8A4/NxdkqD7fT2d6o/9jYkNVdVWX/RCcDrgIl5/lpzVm/LWIETRweS9HJJ+wKXAj/OzQiQag2PR8SzknYGPlBYbBGpev/qQtlwUtPHk5JGs+rk3sgvgO0lvS83vXyCHk4Wkt4qaWK+BrEceJZVTQuP1sVQ1lsK2z4eeA64PU+7G/iApGGS3sPqzQ6PAptJ2riH9V4O7CNpzxzvCXndf+hHjJcAn5S0taSNSIn5sh6u2TRSet9ExIvA94EzJb0CQNJoSf+3l3WPqbt209vxUvMFSRtK2h44Cqhde7kb2FvSppJeSfqb9GQ48BSwTNK2wMfKfMaCK4F9Jb0jx/8VVj939bX++v06nJR0l0jaFPhSxXi6nhNHZ/m5pKWkavvnSNXzowrTPw58Jc/zRdJJEYCIeJrUlvv7fPfJLsCXgZ2AJ0mJ4aqeNhwRjwHvJzVtLCZdMP19D7O/nHRSe4LUvLEY+O887TxguxzDz8p/dK4hXY94gnSR9H35mgTAccB+wBLSdYZ/rjciHiCd0B/M21ytmSki/kKqaX2b9C1+P9Jtz89XiK3mfOBHpDvY/k5KmP+vwvJTgAtznAeVmP8kYDZwu6SngBtJ36QbuZl0K/Ajkh7LZT0eLwX/k7dxE3B6RNQe8vwR8GdgDvBrViWURk4kJaWlpOOit3lfIiJmApNJNaQFpGOgeAdXX+ufwur79Zuk61iPkb58/KpKPJYuXLY6BjMz6yCucZiZWSVOHGZmVokTh5mZVeLEYWZmlXT0A4Cbb755jBs3rtVhmJl1lBkzZjwWESP7u3xHJ45x48Yxffr0VodhZtZRJD3U91w9c1OVmZlV4sRhZmaVOHGYmVklThxmZlaJE4eZmVXixGFmZpU4cZiZWSVOHGZmVokTh5mZVdLRT47b4Bt38i/+OTzntH1KTzOzocOJw16imADAScDMVuemKjMzq8SJw8zMKnFTlTWFr3eYDV2ucZiZWSVOHGZmVokTh5mZVdK0xCFpfUl/lPRnSTMlfTmXby3pDkmzJV0mad1cvl4en52nj2tWbGZm1n/NvDj+HLBHRCyTtA5wq6RfAp8CzoyISyV9FzgaODf/fiIiXivpEOC/gIObGJ+tofrnPcysOzQtcUREAMvy6Dr5J4A9gA/k8guBKaTEsX8eBrgSOFuS8nqshZwgzKyoqdc4JA2TdDewELgB+BuwJCJW5FnmAaPz8GhgLkCe/iSwWYN1TpI0XdL0RYsWNTN8MzNroKmJIyJWRsQOwBhgZ2DbAVjn1IiYEBETRo4cucYxmplZNYNyV1VELAF+A+wKjJBUayIbA8zPw/OBsQB5+sbA4sGIz8zMymvmXVUjJY3IwxsA7wJmkRLIgXm2I4Br8vC0PE6efrOvb5iZtZ9m3lW1BXChpGGkBHV5RFwr6X7gUklfBe4Czsvznwf8SNJs4HHgkCbGZmZm/dTMu6ruAXZsUP4g6XpHffmzwPubFY+ZmQ0Md3JoTef3e5gNLe5yxMzMKnGNw8ysCwzkg7yucZiZWSWucRjgbkXMrDzXOMzMrBInDjMzq8SJw8zMKnHiMDOzSpw4zMysEicOMzOrxInDzMwqceIwM7NKnDjMzKwSJw4zM6vEicPMzCpx4jAzs0qcOMzMrBInDjMzq8SJw8zMKnHiMDOzSpw4zMysEicOMzOrxInDzMwqado7xyWNBS4CRgEBTI2Ib0maAnwUWJRnPSUirsvLfBY4GlgJfCIirm9WfOb3jJtZ/zQtcQArgBMi4k5Jw4EZkm7I086MiNOLM0vaDjgE2B7YErhR0jYRsbKJMZqZWUVNa6qKiAURcWceXgrMAkb3ssj+wKUR8VxE/B2YDezcrPjMzKx/BuUah6RxwI7AHbnoWEn3SDpf0ia5bDQwt7DYPBokGkmTJE2XNH3RokX1k83MrMma2VQFgKSNgJ8Cx0fEU5LOBf6TdN3jP4FvAB8uu76ImApMBZgwYUIMfMRDl69pmNlAaGrikLQOKWlcHBFXAUTEo4Xp3weuzaPzgbGFxcfkMhvCektmc07bZxAjMbOymtZUJUnAecCsiDijUL5FYbb3Avfl4WnAIZLWk7Q1MB74Y7PiMzOz/mlmjePtwIeAeyXdnctOAQ6VtAOpqWoO8B8AETFT0uXA/aQ7sib7jiozs/bTtMQREbcCajDpul6WORU4tVkxdSNf1zCzgeYnx83MrJKm31VlVs+1ILPO5hqHmZlV4sRhZmaVOHGYmVklThxmZlaJE4eZmVXixGFmZpX4dlxrW8Xbdt1vlVn7cI3DzMwqcY3DOoJrH2btwzUOMzOrxInDzMwqceIwM7NKnDjMzKwSJw4zM6vEicPMzCpx4jAzs0qcOMzMrBInDjMzq8SJw8zMKnHiMDOzSpw4zMyskqYlDkljJf1G0v2SZko6LpdvKukGSX/NvzfJ5ZJ0lqTZku6RtFOzYjMzs/4rlTjySf0wSV/M41tJ2rmPxVYAJ0TEdsAuwGRJ2wEnAzdFxHjgpjwOsBcwPv9MAs6t/GnMzKzpytY4vgPsChyax5cC5/S2QEQsiIg78/BSYBYwGtgfuDDPdiFwQB7eH7goktuBEZK2KPtBzMxscJRNHBMjYjLwLEBEPAGsW3YjksYBOwJ3AKMiYkGe9AgwKg+PBuYWFpuXy8zMrI2UTRwvSBoGBICkkcCLZRaUtBHwU+D4iHiqOC0iorbOsiRNkjRd0vRFixZVWdTMzAZA2cRxFnA18ApJpwK3Al/rayFJ65CSxsURcVUufrTWBJV/L8zl84GxhcXH5LLVRMTUiJgQERNGjhxZMnwzMxsopRJHRFwMfAb4OrAAOCAiruhtGUkCzgNmRcQZhUnTgCPy8BHANYXyw/OF+F2AJwtNWmZm1iZKvXM8n8hnRsQ5efzlkiZGxB29LPZ24EPAvZLuzmWnAKcBl0s6GngIOChPuw7YG5gNPA0cVfXDmJlZ85VKHKRbY4vPVSxrULaaiLgVUA+T92wwfwCTS8ZjZmYtUjZxKJ/YAYiIFyWVXdZs0Iw7+Rf/HJ5z2j4tjMRs6Cp78n9Q0idY9VDex4EHmxOSraniydPMbKCVvavqGOBtpLuc5gETSU93m5lZlylV44iIhcAhTY7FzMw6QNm7qkYCHwXGFZeJiA83JywzM2tXZa9xXAP8DrgRWNm8cMyao/66jy+cm/Vf2cSxYUSc1NRIzMysI5RNHNdK2jsirmtqNGYDyHeXmTVH2cRxHHCKpOeB50kP9kVEvLxpkZn1wAnBrLXK3lU1vNmBmJlZZ6j6BsAv5PGxJd4AaGZmQ1DVNwB+II8vo483AJqZ2dBU9hrHxIjYSdJdkN4AKKn0GwDNzGzoKJs4+v0GQBscvmBsZoOlqW8ANDOzoafPGoektYC/k94AuCfpVtwDImJWk2MzM7M21GfiyO/eOCcidgQeGISYzMysjZVtqrpJ0r/n94ibmVkXK5s4/gO4AnhO0lOSlkp6qolxmZlZm/KT42ZmVknZ93Hs1qg8Im4Z2HDMzKzdlX2O49OF4fWBnYEZwB4DHpHZICg+9+J3c5hVU7apar/iuKSxwDebEpGZmbW1sjWOevOA1w9kINY3f0s2s3ZQ9hrHt8ndjZDuxNoBuLOPZc4H9gUWRsQbctkU0rvLF+XZTqm9HErSZ4GjSa+m/UREXF/pk5iZ2aAoW+OYXhheAVwSEb/vY5kLgLOBi+rKz4yI04sFkrYDDgG2B7YEbpS0TUT4/eZmZm2mbOK4Eni2diKXNEzShhHxdE8LRMQtksaVXP/+wKUR8Rzwd0mzSRfgbyu5vJmZDZLST44DGxTGNwBu7Oc2j5V0j6TzJW2Sy0YDcwvzzMtlLyFpkqTpkqYvWrSo0SxmZtZEZRPH+hGxrDaShzfsx/bOBV5DukayAPhG1RVExNSImBARE0aOHNmPEMzMbE2UTRzLJe1UG5H0FuCZqhuLiEcjYmVEvAh8n9QcBTAfGFuYdUwuMzOzNlP2GsfxwBWSHiZ1q/5K4OCqG5O0RUQsyKPvBe7Lw9OAn0g6g3RxfDzwx6rr7yZ+cZOZtUrZBwD/JGlb4HW56C8R8UJvy0i6BNgd2FzSPOBLwO6SdiDd2juH1HkiETFT0uXA/aS7tib7jiozs/ZU9jmOycDFEXFfHt9E0qER8Z2elomIQxsUn9fL/KcCp5aJx8zMWqfsNY6PRsSS2khEPEF6kM/MzLpM2cQxrPgSJ0nDgHWbE5KZmbWzshfHrwcuk/S9PH4M8KvmhGRmZu2sbOL4Aqlp6uN5/Hp6uV5hZmZDV6+JQ9LawNeAo1j1ZPdWwIOkZi7f+WRm1mX6qnH8NzAceHVELAWQNJz0xPfpwHHNDc9scNU/H9NT9/Vl5zMbivq6OL4v6Y6qpbWCPPwxYO9mBmZmZu2pr8QRERENCley6v0cZmbWRfpKHPdLOry+UNJhwAPNCcnMzNpZX9c4JgNXSfowMCOXTSB1q/7eZgZmZmbtqdfEERHzgYmS9iC9nQ/guoi4qemRmZlZWyrbyeHNwM1NjsXMzDpA2QcAzYas3rqoL07zLbdmSdm+qszMzAAnDjMzq8iJw8zMKvE1jjbnV8S2D/8tzBLXOMzMrBInDjMzq8SJw8zMKnHiMDOzSpw4zMysEicOMzOrxLfjmg0wd1NiQ13TahySzpe0UNJ9hbJNJd0g6a/59ya5XJLOkjRb0j2SdmpWXGZmtmaaWeO4ADgbuKhQdjJwU0ScJunkPH4SsBcwPv9MBM7Nv806gh8OtG7StBpHRNwCPF5XvD9wYR6+EDigUH5RJLcDIyRt0azYzMys/wb74vioiFiQhx8BRuXh0cDcwnzzctlLSJokabqk6YsWLWpepGZm1lDL7qqKiACiH8tNjYgJETFh5MiRTYjMzMx6M9iJ49FaE1T+vTCXzwfGFuYbk8vMzKzNDHbimAYckYePAK4plB+e767aBXiy0KRlZmZtpGl3VUm6BNgd2FzSPOBLwGnA5ZKOBh4CDsqzXwfsDcwGngaOalZcZma2ZpqWOCLi0B4m7dlg3gAmNysWMzMbOF3x5Lif5DUzGzhdkTjM2lFvX2j8ZcfamTs5NDOzSpw4zMysEicOMzOrxNc42oDbs4eu+s4P/fe1ocA1DjMzq8Q1jkFS9punu+c2s3bnGoeZmVUyZGsc/uZu7cjHpQ0FrnGYmVklThxmZlaJE4eZmVUyZK9xmHUSX/uwTuLE0UQ+GZjZUOTEYdah/FS6tYoTh1kXc3c31h9OHGYdpGzzpxOCNZMTR4v4+oeV5WPF2o0Th9kQ4QRjg8XPcZiZWSVOHGZmVombqsy6jJu0bE21JHFImgMsBVYCKyJigqRNgcuAccAc4KCIeKIV8ZmZWc9a2VT1zojYISIm5PGTgZsiYjxwUx43M7M2007XOPYHLszDFwIHtDAWMzPrQauucQTwa0kBfC8ipgKjImJBnv4IMKrRgpImAZMAttpqq8GItRK3H1unchcmVlarEsc7ImK+pFcAN0h6oDgxIiInlZfISWYqwIQJExrOY2Zrzk+fW09akjgiYn7+vVDS1cDOwKOStoiIBZK2ABa2IjYzeyknESsa9MQh6WXAWhGxNA+/G/gKMA04Ajgt/75msGMzs+Zx8hk6WlHjGAVcLam2/Z9ExK8k/Qm4XNLRwEPAQS2IzczM+jDoiSMiHgTe3KB8MbDnYMdjZmbVtNPtuGZm1gHc5Ug/uK3WOolvEbeB5hqHmZlV0nU1jv4+5NTTtzZ/m7Nu4wcFzTUOMzOrpOtqHPV8vcKsOfrzfnTw/2EncI3DzMwq6foah5mtmf5c5+tPbcQ1kfbhxNEDX/Q2a19u3motJw4z6xquwQwMJw4zayu+9b39OXEU+MA063z9qVW46asaJw4z63j+0je4nDjMzAZAN10/ceIwM+unbq3pOHGYmdXpptpDfzhxmNmQNRA1AieRl3LiMDMbREMhETlxmFlXGqyuUqDnBFF2fe2WYJw4zMw6SDs8c+LEYWbW5tqtU0h3q25mZpW4xmFm1mTNfN6jFc+SuMZhZmaVtF2NQ9J7gG8Bw4AfRMRpZZft1qc4zczqNfN82FY1DknDgHOAvYDtgEMlbdfaqMzMrKitEgewMzA7Ih6MiOeBS4H9WxyTmZkVtFtT1WhgbmF8HjCxOIOkScCkPPqcpPsGKbZ2tznwWKuDaBPeF6t4X6zifbHK69Zk4XZLHH2KiKnAVABJ0yNiQotDagveF6t4X6zifbGK98UqkqavyfLt1lQ1HxhbGB+Ty8zMrE20W+L4EzBe0taS1gUOAaa1OCYzMytoq6aqiFgh6VjgetLtuOdHxMxeFpk6OJF1BO+LVbwvVvG+WMX7YpU12heKiIEKxMzMukC7NVWZmVmbc+IwM7NKOjZxSHqPpL9Imi3p5FbHM5gkjZX0G0n3S5op6bhcvqmkGyT9Nf/epNWxDgZJwyTdJenaPL61pDvysXFZvtGiK0gaIelKSQ9ImiVp1248LiR9Mv9v3CfpEknrd9NxIel8SQuLz7n1dBwoOSvvl3sk7dTX+jsycbhrElYAJ0TEdsAuwOT8+U8GboqI8cBNebwbHAfMKoz/F3BmRLwWeAI4uiVRtca3gF9FxLbAm0n7pauOC0mjgU8AEyLiDaQbbQ6hu46LC4D31JX1dBzsBYzPP5OAc/taeUcmDrq8a5KIWBARd+bhpaSTw2jSPrgwz3YhcEBrIhw8ksYA+wA/yOMC9gCuzLN0xX4AkLQxsBtwHkBEPB8RS+jC44J0x+gGktYGNgQW0EXHRUTcAjxeV9zTcbA/cFEktwMjJG3R2/o7NXE06ppkdItiaSlJ44AdgTuAURGxIE96BBjVorAG0zeBzwAv5vHNgCURsSKPd9OxsTWwCPhhbrr7gaSX0WXHRUTMB04H/kFKGE8CM+je46Kmp+Og8vm0UxOHAZI2An4KHB8RTxWnRbrPekjfay1pX2BhRMxodSxtYm1gJ+DciNgRWE5ds1SXHBebkL5Fbw1sCbyMlzbbdLU1PQ46NXF0fdckktYhJY2LI+KqXPxorYqZfy9sVXyD5O3Av0maQ2qu3IPUxj8iN1FAdx0b84B5EXFHHr+SlEi67bj4V+DvEbEoIl4AriIdK916XNT0dBxUPp92auLo6q5Jcjv+ecCsiDijMGkacEQePgK4ZrBjG0wR8dmIGBMR40jHwM0R8UHgN8CBebYhvx9qIuIRYK6kWs+newL302XHBamJahdJG+b/ldp+6MrjoqCn42AacHi+u2oX4MlCk1ZDHfvkuKS9Se3bta5JTm1xSING0juA3wH3sqpt/xTSdY7Lga2Ah4CDIqL+AtmQJGl34MSI2FfSq0k1kE2Bu4DDIuK5VsY3WCTtQLpRYF3gQeAo0hfErjouJH0ZOJh0B+JdwEdI7fZdcVxIugTYndSV/KPAl4Cf0eA4yMn1bFJz3tPAURHRa++5HZs4zMysNTq1qcrMzFrEicPMzCpx4jAzs0qcOMzMrBInDjMzq8SJwzqKpJWS7s69nl4hacNB3v4Fkg7se87VljlG0uF5+EhJWzYnOrPB4cRhneaZiNgh93r6PHBMqwPqjaS1I+K7EXFRLjqS1A1Gq+KRJP/f2xrxAWSd7HfAayXtl9+zcJekGyWNApB0b34/hSQtLnzrv0jSu/K3/2sk/Ta/o+BLefq4uvcYnChpSv3GJX1R0p9y7WdqfpCKvL5vSpoOHCdpSl7HgcAE4OJca9pH0s8K63uXpKsbbOc0pXev3CPp9Fw2StLVkv6cf96Wyz+V47lP0vGFz/MXSRcB9wFjJX06x35PfljOrDQnDutIuc+hvUhPz98K7JI79ruU1FsuwO9JfRRtT3qK+l9y+a7AH/LwzsC/A28C3i9pQoUwzo6It+bazwbAvoVp60bEhIj4Rq0gIq4EpgMfjIgdgOuAbSWNzLMcBZxf9zk3A94LbB8RbwK+miedBfxPRLyZ1B/VTElvyeuYSHpPy0cl7ZjnHw98JyK2B16Xx3cGdgDeImm3Cp/bupwTh3WaDSTdTToB/4PUZ9cY4HpJ9wKfJiUKSDWS3fLPucAblV7y80RELM/z3BARiyPiGVJneO+oEMs7c03nXlIHi9sXpl3W18K5h9IfAYdJGkFKaL+sm+1J4FngPEnvI3UJQd7euXk9KyPiyRz71RGxPCKW5c9TS5YP5XctALw7/9wF3AlsS0okZqWs3fcsZm3lmfxt/Z8kfRs4IyKm5T6rpuRJtwCTSX3zfI70zf1AUkKpqe9zJ0j9GxW/VK1fH4Sk9YHvkN4yNzc3ZRXnW16/TA9+CPyclByuKLwvIgUTsULSzqSO+g4EjiUljaqK8Qj4ekR8rx/rMXONw4aEjVnVDXSt908iYi6pk7fxEfEgqUnrRFJCqXmX0ruYNyC9Ee33pE7hXiFpM0nrsXoTVE0tSTym9F6UsndaLQWGF2J8GHgY+Dwpiawmr3vjiLgO+CTpdbCQXv35sTzPMKW3//0OOCD3CvsyUqL8Xf06geuBD+d1I2m0pFeUjN/MNQ4bEqYAV0h6AriZ9AKfmjtIPShDOol+nZRAav5Ieq/JGODHtV5BJX0lT5sPPFC/wYhYIun7pIvNj5C6+i/jAuC7kp4Bds1NZBcDIyNiVoP5hwPX5BqOgE/l8uOAqZKOBlYCH4uI2yRdkOMG+EFE3KX0lshi7L+W9Hrgtnw9fxlwGEP/PR02QNw7rnUtSUeSmpqObXEcZwN3RcR5rYzDrCzXOMxaSNIM0vWHE1odi1lZrnGYmVklvjhuZmaVOHGYmVklThxmZlaJE4eZmVXixGFmZpX8L/PpWaUzne0EAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Create a plot that shows the distribution of the output of the training samples\n",
        "plt.hist(csv_train_data['Pawpularity'], bins=100)\n",
        "plt.title(\"Data distribution of the tabular data\")\n",
        "plt.xlabel(\"Pawpularity score\")\n",
        "plt.ylabel(\"Occurence\")\n",
        "plt.xlim(0, 100)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eccde6f",
      "metadata": {
        "id": "5eccde6f"
      },
      "source": [
        "# Import image data\n",
        "The images are imported from the folders. Each image is reshaped to a 64x64 image. In this way all the images have the same shape and we do not use much memory, to speed up analysis. After the images are imported, the images and their names are shuffled. This is done, so we can later take a validation sample containing a random subsample of the dataset. It could be that the images in the dataset contain some order, so by shuffling we ensure that the subset for the validation data is random.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6a8970b3",
      "metadata": {
        "id": "6a8970b3"
      },
      "outputs": [],
      "source": [
        "def reshape_images(path, n):\n",
        "    \"\"\"\n",
        "    This function returns a list of images, which are reshaped to 64 x 64 \n",
        "    and a list with the names of the images.\n",
        "    \"\"\"\n",
        "    # Set the current path\n",
        "    chdir(path)\n",
        "    \n",
        "    # Preset the lists\n",
        "    images = []\n",
        "    image_names = []\n",
        "    \n",
        "    # Go over all the files in the path\n",
        "    for i in listdir():\n",
        "        \n",
        "        # Get the name of the image, without .jpg\n",
        "        image_names.append(i[:-4])\n",
        "        \n",
        "        # Get the image and reshape to n x n\n",
        "        file = cv2.imread(i)\n",
        "        file = cv2.resize(file,(n, n), interpolation=cv2.INTER_AREA)\n",
        "        \n",
        "        # Rescale the pixels and store in the list\n",
        "        images.append(file/255)\n",
        "        \n",
        "    return images, image_names\n",
        "\n",
        "# Reshape train and test images\n",
        "train_imgs, train_names = reshape_images(\"/tmp/Fitgirls0011-main/petfinder-pawpularity-score/train\", 64)\n",
        "test_imgs, test_names = reshape_images(\"/tmp/Fitgirls0011-main/petfinder-pawpularity-score/test\", 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b818ae9",
      "metadata": {
        "id": "8b818ae9"
      },
      "source": [
        "# Combine tabular data with images\n",
        "To ensure that the dataframe has the same order as the images in the list, we sort the dataframe based on the names of the images. If this would not be the case, it could be that you learn incorrectly, as the output of an image perhaps is not the real output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "85185f99",
      "metadata": {
        "id": "85185f99"
      },
      "outputs": [],
      "source": [
        "def sort_dataframe(data, images, names):\n",
        "    \"\"\"\n",
        "    This function sorts the dataframe of the csv data according to the image names.\n",
        "    \"\"\"\n",
        "    data_sorted = pd.DataFrame()\n",
        "\n",
        "    # Iterate over images and get index of each image\n",
        "    for img, name in zip(images, names):\n",
        "        location = data[data['Id'] == name].index[0]\n",
        "\n",
        "        # Sort dataframe according to index of images\n",
        "        data_sorted = data_sorted.append([data.loc[location]])\n",
        "\n",
        "        # Reset the index of the dataframe\n",
        "        data_sorted = data_sorted.reset_index().drop(['index'],axis=1)\n",
        "        \n",
        "    return data_sorted\n",
        "\n",
        "# Sort training and testing data\n",
        "train_data_sorted = sort_dataframe(csv_train_data, train_imgs, train_names)\n",
        "test_data_sorted = sort_dataframe(csv_test_data, test_imgs, test_names)\n",
        "sample_submission_sorted = sort_dataframe(sample_submission, test_imgs, test_names)\n",
        "\n",
        "# train_data_sorted.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9206b4b",
      "metadata": {
        "id": "e9206b4b"
      },
      "source": [
        "# Processing data\n",
        "The tabular data is split in x and y values and converted to numpy arrays, so the neural network can handle the data. Moreover, the image data is converted to numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove samples with pawpularity score of 100\n",
        "indexNames = train_data_sorted[train_data_sorted['Pawpularity'] == 100].index\n",
        "train_data_new = train_data_sorted.drop(indexNames)\n",
        "train_imgs_new = np.delete(train_imgs, indexNames, axis=0)"
      ],
      "metadata": {
        "id": "CxecT9dhMfbF"
      },
      "id": "CxecT9dhMfbF",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select x-values (the 12 input features) and y-values from training data\n",
        "x_tabular = train_data_new.iloc[:,1:13].to_numpy()\n",
        "y = train_data_new.iloc[:,13].to_numpy()\n",
        "\n",
        "# Select x (the 12 input features) and y (pawpularity) values from testing data\n",
        "x_test_tabular = test_data_sorted.iloc[:,1:13].to_numpy()\n",
        "y_test = sample_submission_sorted.iloc[:,1].to_numpy()\n",
        "\n",
        "# Create numpy array of image data \n",
        "x_images = np.array(train_imgs_new)\n",
        "test_imgs_array = np.array(test_imgs)"
      ],
      "metadata": {
        "id": "Agl1YxFwNcKo"
      },
      "id": "Agl1YxFwNcKo",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "RNE8NWL7xgmp",
      "metadata": {
        "id": "RNE8NWL7xgmp"
      },
      "source": [
        "# Create seperate neural networks\n",
        "We create a tabular neural network to handle the data in the csv. Then we create a convolutional neural network to handle the image data. Both neural networks have no output layer, since they will be concatenated to one neural network, which will give the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ccf1cb99",
      "metadata": {
        "id": "ccf1cb99"
      },
      "outputs": [],
      "source": [
        "def build_neural_net(input_size, hidden_nodes):\n",
        "    \"\"\"\n",
        "    Build neural network with an input size and a hidden layer with a number of \n",
        "    hidden nodes.\n",
        "    \"\"\"\n",
        "    # Create a sequential model object\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Create hidden layer with relu activations\n",
        "    model.add(layers.Dense(units=hidden_nodes, activation='relu', input_shape=(input_size,)))    \n",
        "\n",
        "    # Create hidden layer with relu activations\n",
        "    model.add(layers.Dropout(0.4))\n",
        "    model.add(layers.Dense(units=hidden_nodes, activation=\"relu\"))\n",
        "\n",
        "    # Create hidden layer with relu activations\n",
        "    model.add(layers.Dropout(0.4))\n",
        "    model.add(layers.Dense(units=hidden_nodes, activation=\"relu\"))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create neural network for tabular data and get summary of model \n",
        "# with 12 inputs and 100 hidden nodes\n",
        "tabular_NN = build_neural_net(12, hidden_nodes=20)\n",
        "# tabular_NN.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bd18b2f9",
      "metadata": {
        "id": "bd18b2f9"
      },
      "outputs": [],
      "source": [
        "def build_convol_net(image_size, hidden_nodes):\n",
        "    \"\"\"\n",
        "    Build neural network with an input size and a hidden layer with a number \n",
        "    of hidden nodes.\n",
        "    \"\"\"\n",
        "    # Create a sequential model object\n",
        "    model = models.Sequential()\n",
        "    \n",
        "    # Create a convolutional layer with relu activation\n",
        "    model.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', input_shape=image_size, padding='same'))\n",
        "    model.add(layers.MaxPool2D(pool_size=(2,2), strides=2))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    # Create a convolutional layer with relu activation\n",
        "    model.add(layers.Dropout(0.4))\n",
        "    model.add(layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "    model.add(layers.MaxPool2D(pool_size=(2,2), strides=2))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    # Create a convolutional layer with relu activation\n",
        "    model.add(layers.Dropout(0.4))\n",
        "    model.add(layers.Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "    model.add(layers.MaxPool2D(pool_size=(2,2), strides=2))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    \n",
        "    # Create a flattening layer\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Create a dense layer with relu activations\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    model.add(layers.Dense(units=hidden_nodes, activation=\"relu\", \n",
        "              kernel_regularizer=regularizers.l2(1e-3),\n",
        "              bias_regularizer=regularizers.l2(1e-3),\n",
        "              activity_regularizer=regularizers.l2(1e-3)))\n",
        "\n",
        "    # Create a dense layer with relu activations\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    model.add(layers.Dense(units=hidden_nodes, activation=\"relu\", \n",
        "              kernel_regularizer=regularizers.l2(1e-3),\n",
        "              bias_regularizer=regularizers.l2(1e-3),\n",
        "              activity_regularizer=regularizers.l2(1e-3)))\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7adc6959",
      "metadata": {
        "id": "7adc6959"
      },
      "outputs": [],
      "source": [
        "# Create neural network for image data and get summary of model\n",
        "image_size = (64, 64, 3)\n",
        "image_NN = build_convol_net(image_size, hidden_nodes=20)\n",
        "# image_NN.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vtZpIRuAue4B",
      "metadata": {
        "id": "vtZpIRuAue4B"
      },
      "source": [
        "## Concatenate tabular and image data models\n",
        "Concatenate the tabular and image models to create one neural network that can handle both types of data. This neural network will give the prediction of the pawpularity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ZKkweNdcNXLl",
      "metadata": {
        "id": "ZKkweNdcNXLl"
      },
      "outputs": [],
      "source": [
        "def linear_limit(x):\n",
        "    \"\"\"\n",
        "    Create a linear activation function that clips the output at 0 and 100.\n",
        "    \"\"\"\n",
        "    activation_x = activations.linear(x)\n",
        "    activation_x_new = K.clip(activation_x, 0, 100)\n",
        "\n",
        "    return activation_x_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1441c455",
      "metadata": {
        "id": "1441c455"
      },
      "outputs": [],
      "source": [
        "def concatenate_models(model1, model2, hidden_nodes):\n",
        "    \"\"\"\n",
        "    Concatenate two neural network models, model1 and model2, and create\n",
        "    a concatenated model with dense layers with some hidden nodes.\n",
        "    \"\"\"\n",
        "    # Input for concatenated model is retrieved by concatenating the output\n",
        "    # of both models\n",
        "    concat_input = layers.concatenate([model1.output, model2.output])\n",
        "\n",
        "    # Create hidden layer with relu activation\n",
        "    hidden_layer_1 = layers.Dense(hidden_nodes, activation=\"relu\", \n",
        "              kernel_regularizer=regularizers.l2(1e-1),\n",
        "              bias_regularizer=regularizers.l2(1e-1),\n",
        "              activity_regularizer=regularizers.l2(1e-1))(concat_input)\n",
        "\n",
        "    # Create hidden layer with relu activation\n",
        "    drop_out_1 = layers.Dropout(0.4)(hidden_layer_1)    \n",
        "    hidden_layer_2 = layers.Dense(hidden_nodes, activation=\"relu\", \n",
        "              kernel_regularizer=regularizers.l2(1e-1),\n",
        "              bias_regularizer=regularizers.l2(1e-1),\n",
        "              activity_regularizer=regularizers.l2(1e-1))(drop_out_1)\n",
        "\n",
        "    # Create hidden layer with relu activation\n",
        "    drop_out_2 = layers.Dropout(0.4)(hidden_layer_2)\n",
        "    hidden_layer_3 = layers.Dense(hidden_nodes, activation=\"relu\", \n",
        "              kernel_regularizer=regularizers.l2(1e-1),\n",
        "              bias_regularizer=regularizers.l2(1e-1),\n",
        "              activity_regularizer=regularizers.l2(1e-1))(drop_out_2)\n",
        "\n",
        "    # Create output layer\n",
        "    output_layer = layers.Dense(1, activation=linear_limit)(hidden_layer_3)\n",
        "\n",
        "    # Create concatenated model with inputs of both models and output of the\n",
        "    # concatenated model\n",
        "    concat_model = models.Model(inputs=[model1.input, model2.input], outputs=output_layer)\n",
        "\n",
        "    return concat_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "DZ58i65PvawJ",
      "metadata": {
        "id": "DZ58i65PvawJ"
      },
      "outputs": [],
      "source": [
        "# Part of code from: https://www.tensorflow.org/tutorials/keras/regression\n",
        "\n",
        "def plot_loss(history):\n",
        "    \"\"\"\n",
        "    Plot loss during epochs of training a neural network.\n",
        "    \"\"\"\n",
        "    \n",
        "    fig, axs = plt.subplots(1,2,figsize=(20,5))\n",
        "\n",
        "    ylimits = [[0, 1000], [0, 30]]\n",
        "\n",
        "    for i, metric in enumerate(['loss', 'root_mean_squared_error']):\n",
        "        axs[i].plot(history.history[metric])\n",
        "        axs[i].plot(history.history['val_'+metric])\n",
        "        axs[i].legend(['training', 'validation'], loc='best')\n",
        "\n",
        "        axs[i].set_title('Model '+metric)\n",
        "        axs[i].set_ylabel(metric)\n",
        "        axs[i].set_xlabel('epoch')\n",
        "        axs[i].set_ylim(ylimits[i])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def train_and_evaluate(model, image_x, tabular_x, train_y, x_val_tabular, x_val_imgs, val_y, epochs=20, preprocess = {}, augment={}):\n",
        "    \"\"\"\n",
        "    This function trains and evaluated a model. It first compiles the model with \n",
        "    the loss and metrics. It then makes a train and validation generator for the \n",
        "    image data, based on the preprocess and augment input. \n",
        "    It then trains the model on both the image and tabular data for epochs times. \n",
        "    The values of the loss and metric are plotted and printed.\n",
        "    \"\"\"\n",
        "\n",
        "    # Compile model and use mean squared error as loss and root mean squared error as metric\n",
        "    model.compile(loss=MeanSquaredError(), metrics=[RootMeanSquaredError()])\n",
        "\n",
        "    # Preprocess the image data\n",
        "    train_gen = preprocessing.image.ImageDataGenerator(**preprocess, **augment)\n",
        "    train_gen.fit(image_x)\n",
        "\n",
        "    val_gen = preprocessing.image.ImageDataGenerator(**preprocess)\n",
        "    val_gen.fit(image_x)\n",
        "\n",
        "    # Train the model by fitting both tabular and image data at the same time\n",
        "    history = model.fit(train_gen.flow([image_x, tabular_x], train_y), epochs = epochs, validation_data=val_gen.flow([x_val_imgs, x_val_tabular], val_y))\n",
        "\n",
        "    # Plot the loss and metric\n",
        "    # plot_loss(history)\n",
        "\n",
        "\n",
        "\n",
        "    # print(f\"Validation Accuracy: {model.evaluate(val_gen.flow([x_val_imgs, x_val_tabular], val_y))[1]}\")\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7eJp3actWxsB",
      "metadata": {
        "id": "7eJp3actWxsB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "d02cbe0c-0a66-4ed3-ff59-ad25da3a1fb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "192/241 [======================>.......] - ETA: 2s - loss: 928.0919 - root_mean_squared_error: 27.5471"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-7c98dffc4deb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m avg_train_loss, avg_val_loss, loss_df = k_fold(5, x_tabular, y, x_images, epochs=60, \n\u001b[1;32m    105\u001b[0m                                                \u001b[0mpreprocess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'featurewise_center'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'featurewise_std_normalization'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                                                augment={'rotation_range': 90, 'horizontal_flip': True, 'shear_range': 0.2})\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;31m# display(loss_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-7c98dffc4deb>\u001b[0m in \u001b[0;36mk_fold\u001b[0;34m(num_folds, x_tabular, y, x_imgs, epochs, preprocess, augment)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mconcat_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_NN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtabular_NN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         history = train_and_evaluate(concat_model, x_imgs_train, x_tabular_train, \n\u001b[0;32m---> 49\u001b[0;31m                            y_train, x_tabular_val, x_imgs_val, y_val_2, epochs=epochs, preprocess=preprocess, augment=augment)\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Add the evaluation loss each time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-7d1b4ff67fb6>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, image_x, tabular_x, train_y, x_val_tabular, x_val_imgs, val_y, epochs, preprocess, augment)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Train the model by fitting both tabular and image data at the same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtabular_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_val_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val_tabular\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Plot the loss and metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def k_fold(num_folds, x_tabular, y, x_imgs, epochs=20, preprocess={}, augment={}):\n",
        "    \"\"\"\n",
        "    Train and evaluate the data for num-folds times, and return the average \n",
        "    training and validation loss. First the data is split in num-folds batches\n",
        "    and then the model is trained on the data, where a different batch is the \n",
        "    validation data each time.\n",
        "    \"\"\"\n",
        "    losses = {}\n",
        "\n",
        "    # Create kfold object to later split the data\n",
        "    kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "    x_indices = np.array(range(len(x_tabular)))\n",
        "    y_indices = np.array(range(len(y)))\n",
        "\n",
        "    # Preset arrays to add the loss for training and validation\n",
        "    train_loss1 = np.array([0.0, 0.0])\n",
        "    val_loss1 = np.array([0.0, 0.0])\n",
        "\n",
        "    # Train and evaluate the model for num-fold times on a different training \n",
        "    # and validation set each time\n",
        "    count = 0 \n",
        "\n",
        "    train_loss = np.array(np.zeros(epochs))\n",
        "    train_acc = np.array(np.zeros(epochs))\n",
        "    val_loss = np.array(np.zeros(epochs))\n",
        "    val_acc = np.array(np.zeros(epochs))\n",
        "    \n",
        "    for id_train, id_val in kfold.split(x_indices, y_indices):\n",
        "        \n",
        "        # Make Neural Networks before concatenation\n",
        "        tabular_NN = build_neural_net(12, hidden_nodes=20)\n",
        "        image_size = (64, 64, 3)\n",
        "        image_NN = build_convol_net(image_size, hidden_nodes=20)\n",
        "\n",
        "        # Create subset training and validation data\n",
        "        x_tabular_train = x_tabular[id_train]\n",
        "        x_imgs_train = x_imgs[id_train]\n",
        "        y_train = y[id_train]\n",
        "        \n",
        "        x_tabular_val = x_tabular[id_val]\n",
        "        x_imgs_val = x_imgs[id_val]\n",
        "        y_val_2 = y[id_val]\n",
        "        \n",
        "        # Train and evaluate the model\n",
        "        concat_model = concatenate_models(image_NN, tabular_NN, hidden_nodes=20)\n",
        "        history = train_and_evaluate(concat_model, x_imgs_train, x_tabular_train, \n",
        "                           y_train, x_tabular_val, x_imgs_val, y_val_2, epochs=epochs, preprocess=preprocess, augment=augment)\n",
        "\n",
        "        # Add the evaluation loss each time\n",
        "        train_loss1 += concat_model.evaluate([x_imgs_train, x_tabular_train], y_train)\n",
        "        val_loss1 += concat_model.evaluate([x_imgs_val, x_tabular_val], y_val_2)\n",
        "        \n",
        "        # print(history.history['loss'])\n",
        "\n",
        "        # Add all the losses and metrics\n",
        "        train_loss += history.history['loss']\n",
        "        train_acc += history.history['root_mean_squared_error']\n",
        "        val_loss += history.history['val_loss']\n",
        "        val_acc += history.history['val_root_mean_squared_error']\n",
        "\n",
        "        losses[count] = history.history\n",
        "        count += 1 \n",
        "\n",
        "    # Calculate average loss and metric\n",
        "    avg_train_loss = train_loss / num_folds\n",
        "    avg_val_loss = val_loss / num_folds\n",
        "    avg_train_acc = train_acc / num_folds\n",
        "    avg_val_acc = val_acc / num_folds\n",
        "\n",
        "    print(train_loss, avg_train_loss)\n",
        "\n",
        "    # Calculate average evaluation\n",
        "    avg_train_loss1 = train_loss / num_folds\n",
        "    avg_val_loss1 = val_loss / num_folds\n",
        "\n",
        "    fig, axs = plt.subplots(1,2,figsize=(20,5)) \n",
        "\n",
        "    # Plot MSE\n",
        "    axs[0].plot(avg_train_loss)\n",
        "    axs[0].plot(avg_val_loss)\n",
        "    axs[0].legend(['training', 'validation'], loc='best')\n",
        "    \n",
        "    axs[0].set_title('Model MSE')\n",
        "    axs[0].set_ylabel('MSE')\n",
        "    axs[0].set_xlabel('epoch')\n",
        "    axs[0].set_ylim([0, 1000])\n",
        "\n",
        "    # Plot RMSE\n",
        "    axs[1].plot(avg_train_acc)\n",
        "    axs[1].plot(avg_val_acc)\n",
        "    axs[1].legend(['training', 'validation'], loc='best')\n",
        "    \n",
        "    axs[1].set_title('Model RMSE')\n",
        "    axs[1].set_ylabel('RMSE')\n",
        "    axs[1].set_xlabel('epoch')\n",
        "    axs[1].set_ylim([0, 30])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return avg_train_loss1, avg_val_loss1, losses\n",
        "\n",
        "avg_train_loss, avg_val_loss, loss_df = k_fold(5, x_tabular, y, x_images, epochs=60, \n",
        "                                               preprocess={'featurewise_center': True, 'featurewise_std_normalization': True},\n",
        "                                               augment={'rotation_range': 90, 'horizontal_flip': True, 'shear_range': 0.2})\n",
        "# display(loss_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The average training loss is {avg_train_loss[0]} and the average training RMSE is {avg_train_loss[1]}')\n",
        "print(f'The average validation loss is {avg_val_loss[0]} and the average validation RMSE is {avg_val_loss[1]}')\n",
        "\n",
        "# See how the model scored on the testing data\n",
        "# loss = concat_model.evaluate([test_imgs_array, x_test_tabular], y_test)\n",
        "# print(f'Test loss: {loss}')"
      ],
      "metadata": {
        "id": "HlZsEhYcKBjb"
      },
      "id": "HlZsEhYcKBjb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(loss_df)\n",
        "\n",
        "# training loss\n",
        "print((210.37730407714844 + 188.5962371826172 + 219.09384155273438 + 221.7864532470703 + 205.56365966796875) / 5)\n",
        "\n",
        "# training rmse\n",
        "print((14.113140106201172 + 13.300092697143555 + 14.40040111541748 + 14.513035774230957 + 13.936162948608398) / 5)\n",
        "\n",
        "# val loss\n",
        "print((389.2145080566406 + 360.23553466796875 + 410.3980712890625 + 390.0919494628906 + 365.04302978515625) / 5)\n",
        "\n",
        "# val rmse\n",
        "print((19.471378326416016 + 18.712196350097656 +  19.97732162475586 + 19.4920654296875 + 18.84255027770996) / 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ViyLErBUa3Nt",
        "outputId": "7fedee39-59a5-4208-cc11-0239e23d96f4"
      },
      "id": "ViyLErBUa3Nt",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{0: {'loss': [837.3381958007812,\n",
              "   608.2730102539062,\n",
              "   529.0963134765625,\n",
              "   452.6883239746094,\n",
              "   405.3319396972656,\n",
              "   384.3591613769531,\n",
              "   370.57659912109375,\n",
              "   364.0870056152344,\n",
              "   359.9442138671875,\n",
              "   354.2879638671875,\n",
              "   353.4075012207031,\n",
              "   350.6619873046875,\n",
              "   348.76177978515625,\n",
              "   344.79876708984375,\n",
              "   340.13348388671875,\n",
              "   342.26983642578125,\n",
              "   340.79510498046875,\n",
              "   338.8673095703125,\n",
              "   337.5625305175781,\n",
              "   338.0965270996094,\n",
              "   335.2046813964844,\n",
              "   333.00360107421875,\n",
              "   332.9183349609375,\n",
              "   331.3728942871094,\n",
              "   331.96832275390625,\n",
              "   329.0176086425781,\n",
              "   328.568359375,\n",
              "   327.9915466308594,\n",
              "   325.6312255859375,\n",
              "   325.5157165527344,\n",
              "   325.7080383300781,\n",
              "   322.7975158691406,\n",
              "   322.0274963378906,\n",
              "   319.8836975097656,\n",
              "   317.1188049316406,\n",
              "   315.71319580078125,\n",
              "   313.7970886230469,\n",
              "   310.80828857421875,\n",
              "   306.87689208984375,\n",
              "   302.08258056640625,\n",
              "   298.0513916015625,\n",
              "   298.3238525390625,\n",
              "   290.0969543457031,\n",
              "   288.6943664550781,\n",
              "   283.1955871582031,\n",
              "   273.76629638671875,\n",
              "   276.2306213378906,\n",
              "   268.6248779296875,\n",
              "   264.47119140625,\n",
              "   262.64581298828125,\n",
              "   256.11431884765625,\n",
              "   253.90196228027344,\n",
              "   248.54539489746094,\n",
              "   242.73492431640625,\n",
              "   245.18569946289062,\n",
              "   236.861328125,\n",
              "   237.3797149658203,\n",
              "   234.7405242919922,\n",
              "   224.1094207763672,\n",
              "   221.93003845214844],\n",
              "  'root_mean_squared_error': [25.97297477722168,\n",
              "   22.64067840576172,\n",
              "   21.61383819580078,\n",
              "   20.29604148864746,\n",
              "   19.389270782470703,\n",
              "   18.959659576416016,\n",
              "   18.685667037963867,\n",
              "   18.567964553833008,\n",
              "   18.501819610595703,\n",
              "   18.369993209838867,\n",
              "   18.380126953125,\n",
              "   18.3346004486084,\n",
              "   18.291288375854492,\n",
              "   18.207544326782227,\n",
              "   18.098609924316406,\n",
              "   18.17058563232422,\n",
              "   18.142303466796875,\n",
              "   18.097061157226562,\n",
              "   18.067209243774414,\n",
              "   18.089466094970703,\n",
              "   18.01624298095703,\n",
              "   17.95973777770996,\n",
              "   17.962461471557617,\n",
              "   17.921232223510742,\n",
              "   17.944198608398438,\n",
              "   17.86430549621582,\n",
              "   17.851261138916016,\n",
              "   17.840045928955078,\n",
              "   17.773914337158203,\n",
              "   17.770463943481445,\n",
              "   17.776187896728516,\n",
              "   17.696001052856445,\n",
              "   17.67790985107422,\n",
              "   17.608152389526367,\n",
              "   17.530771255493164,\n",
              "   17.491382598876953,\n",
              "   17.439258575439453,\n",
              "   17.347087860107422,\n",
              "   17.23397445678711,\n",
              "   17.097322463989258,\n",
              "   16.978557586669922,\n",
              "   16.98360252380371,\n",
              "   16.733566284179688,\n",
              "   16.691375732421875,\n",
              "   16.525653839111328,\n",
              "   16.233047485351562,\n",
              "   16.30315399169922,\n",
              "   16.06559944152832,\n",
              "   15.939212799072266,\n",
              "   15.880861282348633,\n",
              "   15.669672012329102,\n",
              "   15.60110092163086,\n",
              "   15.426069259643555,\n",
              "   15.233308792114258,\n",
              "   15.313118934631348,\n",
              "   15.036018371582031,\n",
              "   15.056131362915039,\n",
              "   14.964385986328125,\n",
              "   14.603734970092773,\n",
              "   14.526041984558105],\n",
              "  'val_loss': [797.4779663085938,\n",
              "   615.5339965820312,\n",
              "   457.9284973144531,\n",
              "   361.3736267089844,\n",
              "   346.3787841796875,\n",
              "   333.21331787109375,\n",
              "   329.30828857421875,\n",
              "   326.7870178222656,\n",
              "   329.60784912109375,\n",
              "   323.9170227050781,\n",
              "   321.21307373046875,\n",
              "   321.1138000488281,\n",
              "   320.6661071777344,\n",
              "   319.70465087890625,\n",
              "   319.1643981933594,\n",
              "   318.4395446777344,\n",
              "   318.3356018066406,\n",
              "   320.3302307128906,\n",
              "   317.58477783203125,\n",
              "   317.3321838378906,\n",
              "   320.5979309082031,\n",
              "   317.1214904785156,\n",
              "   318.038818359375,\n",
              "   317.65667724609375,\n",
              "   322.0354919433594,\n",
              "   319.2548522949219,\n",
              "   324.18304443359375,\n",
              "   333.15191650390625,\n",
              "   323.0716552734375,\n",
              "   318.61126708984375,\n",
              "   316.6083068847656,\n",
              "   323.9784240722656,\n",
              "   321.43914794921875,\n",
              "   320.63104248046875,\n",
              "   320.3053894042969,\n",
              "   325.94622802734375,\n",
              "   325.1414794921875,\n",
              "   325.76910400390625,\n",
              "   337.14349365234375,\n",
              "   330.8070983886719,\n",
              "   336.22802734375,\n",
              "   334.955322265625,\n",
              "   337.2530822753906,\n",
              "   331.95538330078125,\n",
              "   337.8650207519531,\n",
              "   342.3094787597656,\n",
              "   342.8360595703125,\n",
              "   342.2615051269531,\n",
              "   351.9430236816406,\n",
              "   348.7346496582031,\n",
              "   360.58099365234375,\n",
              "   354.6807861328125,\n",
              "   359.3757019042969,\n",
              "   350.1712341308594,\n",
              "   354.0675964355469,\n",
              "   352.1833190917969,\n",
              "   348.3759460449219,\n",
              "   344.94744873046875,\n",
              "   354.1544189453125,\n",
              "   357.54681396484375],\n",
              "  'val_root_mean_squared_error': [27.65883445739746,\n",
              "   24.206815719604492,\n",
              "   20.701248168945312,\n",
              "   18.309141159057617,\n",
              "   17.992691040039062,\n",
              "   17.560680389404297,\n",
              "   17.62212562561035,\n",
              "   17.58997917175293,\n",
              "   17.712011337280273,\n",
              "   17.554059982299805,\n",
              "   17.500078201293945,\n",
              "   17.533533096313477,\n",
              "   17.51513671875,\n",
              "   17.524494171142578,\n",
              "   17.518489837646484,\n",
              "   17.509563446044922,\n",
              "   17.521181106567383,\n",
              "   17.589584350585938,\n",
              "   17.50155258178711,\n",
              "   17.516233444213867,\n",
              "   17.616247177124023,\n",
              "   17.51849937438965,\n",
              "   17.54410171508789,\n",
              "   17.548707962036133,\n",
              "   17.673789978027344,\n",
              "   17.600521087646484,\n",
              "   17.739538192749023,\n",
              "   17.961706161499023,\n",
              "   17.71534538269043,\n",
              "   17.582551956176758,\n",
              "   17.530107498168945,\n",
              "   17.7302303314209,\n",
              "   17.66419219970703,\n",
              "   17.64044761657715,\n",
              "   17.63551139831543,\n",
              "   17.773178100585938,\n",
              "   17.776687622070312,\n",
              "   17.787242889404297,\n",
              "   18.098390579223633,\n",
              "   17.926097869873047,\n",
              "   18.076995849609375,\n",
              "   18.037155151367188,\n",
              "   18.10722541809082,\n",
              "   17.962200164794922,\n",
              "   18.11711311340332,\n",
              "   18.240827560424805,\n",
              "   18.250398635864258,\n",
              "   18.23806381225586,\n",
              "   18.49593734741211,\n",
              "   18.411760330200195,\n",
              "   18.726112365722656,\n",
              "   18.56639289855957,\n",
              "   18.696271896362305,\n",
              "   18.450990676879883,\n",
              "   18.546045303344727,\n",
              "   18.500606536865234,\n",
              "   18.39777374267578,\n",
              "   18.30588722229004,\n",
              "   18.555458068847656,\n",
              "   18.64979362487793]},\n",
              " 1: {'loss': [791.187744140625,\n",
              "   579.2650756835938,\n",
              "   503.3294982910156,\n",
              "   428.9117431640625,\n",
              "   393.4330139160156,\n",
              "   373.29754638671875,\n",
              "   362.86822509765625,\n",
              "   353.9955139160156,\n",
              "   349.4798583984375,\n",
              "   345.9832763671875,\n",
              "   343.02294921875,\n",
              "   343.4315490722656,\n",
              "   337.44207763671875,\n",
              "   338.7181091308594,\n",
              "   338.5801086425781,\n",
              "   335.12127685546875,\n",
              "   332.9395446777344,\n",
              "   333.284423828125,\n",
              "   330.1348571777344,\n",
              "   330.588623046875,\n",
              "   327.4436340332031,\n",
              "   327.5670166015625,\n",
              "   326.36517333984375,\n",
              "   323.7862854003906,\n",
              "   322.411865234375,\n",
              "   322.90155029296875,\n",
              "   319.8387451171875,\n",
              "   318.52471923828125,\n",
              "   313.424072265625,\n",
              "   311.48309326171875,\n",
              "   308.6754455566406,\n",
              "   303.6361083984375,\n",
              "   298.7609558105469,\n",
              "   294.23040771484375,\n",
              "   285.26654052734375,\n",
              "   282.66229248046875,\n",
              "   275.18701171875,\n",
              "   269.0392761230469,\n",
              "   265.8321228027344,\n",
              "   259.7275085449219,\n",
              "   253.46226501464844,\n",
              "   250.5620880126953,\n",
              "   248.016845703125,\n",
              "   241.0280303955078,\n",
              "   238.46832275390625,\n",
              "   238.00656127929688,\n",
              "   232.59176635742188,\n",
              "   226.54588317871094,\n",
              "   226.94577026367188,\n",
              "   223.73251342773438,\n",
              "   217.89486694335938,\n",
              "   220.37152099609375,\n",
              "   210.8547821044922,\n",
              "   206.09474182128906,\n",
              "   209.15977478027344,\n",
              "   202.7093505859375,\n",
              "   200.52120971679688,\n",
              "   198.98056030273438,\n",
              "   196.95913696289062,\n",
              "   195.45028686523438],\n",
              "  'root_mean_squared_error': [25.427221298217773,\n",
              "   22.176393508911133,\n",
              "   21.132970809936523,\n",
              "   19.83645248413086,\n",
              "   19.12859535217285,\n",
              "   18.72277069091797,\n",
              "   18.51471710205078,\n",
              "   18.318458557128906,\n",
              "   18.235807418823242,\n",
              "   18.162811279296875,\n",
              "   18.113101959228516,\n",
              "   18.144075393676758,\n",
              "   18.001815795898438,\n",
              "   18.053041458129883,\n",
              "   18.062135696411133,\n",
              "   17.978729248046875,\n",
              "   17.933273315429688,\n",
              "   17.94778060913086,\n",
              "   17.87034797668457,\n",
              "   17.890012741088867,\n",
              "   17.809215545654297,\n",
              "   17.810331344604492,\n",
              "   17.785924911499023,\n",
              "   17.713314056396484,\n",
              "   17.67363929748535,\n",
              "   17.691389083862305,\n",
              "   17.596181869506836,\n",
              "   17.55914306640625,\n",
              "   17.412744522094727,\n",
              "   17.354949951171875,\n",
              "   17.26951789855957,\n",
              "   17.121307373046875,\n",
              "   16.97817039489746,\n",
              "   16.845237731933594,\n",
              "   16.57232666015625,\n",
              "   16.490108489990234,\n",
              "   16.25836944580078,\n",
              "   16.069971084594727,\n",
              "   15.966914176940918,\n",
              "   15.77618408203125,\n",
              "   15.572978973388672,\n",
              "   15.483123779296875,\n",
              "   15.3985595703125,\n",
              "   15.16739559173584,\n",
              "   15.082758903503418,\n",
              "   15.067358016967773,\n",
              "   14.885275840759277,\n",
              "   14.681053161621094,\n",
              "   14.690481185913086,\n",
              "   14.588415145874023,\n",
              "   14.387191772460938,\n",
              "   14.475114822387695,\n",
              "   14.138917922973633,\n",
              "   13.964943885803223,\n",
              "   14.076949119567871,\n",
              "   13.844712257385254,\n",
              "   13.761101722717285,\n",
              "   13.704752922058105,\n",
              "   13.62771987915039,\n",
              "   13.57750415802002],\n",
              "  'val_loss': [694.987548828125,\n",
              "   673.2150268554688,\n",
              "   413.05609130859375,\n",
              "   388.8560485839844,\n",
              "   367.3473815917969,\n",
              "   366.2440490722656,\n",
              "   347.3894348144531,\n",
              "   346.49407958984375,\n",
              "   345.8106384277344,\n",
              "   343.3858642578125,\n",
              "   342.6559753417969,\n",
              "   343.19805908203125,\n",
              "   340.6822204589844,\n",
              "   340.81280517578125,\n",
              "   340.61083984375,\n",
              "   344.97076416015625,\n",
              "   343.5934143066406,\n",
              "   342.82843017578125,\n",
              "   339.71209716796875,\n",
              "   339.953125,\n",
              "   344.7196044921875,\n",
              "   341.8446044921875,\n",
              "   341.1769714355469,\n",
              "   356.3784484863281,\n",
              "   340.6634826660156,\n",
              "   340.8354797363281,\n",
              "   346.2626037597656,\n",
              "   346.5958251953125,\n",
              "   344.5992736816406,\n",
              "   347.4931640625,\n",
              "   352.589599609375,\n",
              "   350.4500732421875,\n",
              "   356.95123291015625,\n",
              "   354.5046081542969,\n",
              "   371.33343505859375,\n",
              "   358.02117919921875,\n",
              "   357.7496337890625,\n",
              "   372.96807861328125,\n",
              "   376.8058166503906,\n",
              "   374.1048889160156,\n",
              "   372.7874755859375,\n",
              "   385.6431884765625,\n",
              "   373.4526672363281,\n",
              "   393.66583251953125,\n",
              "   385.2860107421875,\n",
              "   387.8008117675781,\n",
              "   398.2377624511719,\n",
              "   394.1680603027344,\n",
              "   391.2770690917969,\n",
              "   400.51605224609375,\n",
              "   387.12841796875,\n",
              "   390.7298889160156,\n",
              "   430.33221435546875,\n",
              "   386.71356201171875,\n",
              "   384.5799560546875,\n",
              "   391.8067321777344,\n",
              "   412.74237060546875,\n",
              "   395.5516052246094,\n",
              "   393.07275390625,\n",
              "   404.96923828125],\n",
              "  'val_root_mean_squared_error': [25.22544288635254,\n",
              "   25.431278228759766,\n",
              "   19.52775764465332,\n",
              "   19.018138885498047,\n",
              "   18.616701126098633,\n",
              "   18.66990089416504,\n",
              "   18.13627052307129,\n",
              "   18.162994384765625,\n",
              "   18.156606674194336,\n",
              "   18.116249084472656,\n",
              "   18.11993408203125,\n",
              "   18.15299415588379,\n",
              "   18.10394287109375,\n",
              "   18.130653381347656,\n",
              "   18.126270294189453,\n",
              "   18.27556610107422,\n",
              "   18.247520446777344,\n",
              "   18.232044219970703,\n",
              "   18.145519256591797,\n",
              "   18.157373428344727,\n",
              "   18.29355812072754,\n",
              "   18.21070098876953,\n",
              "   18.206430435180664,\n",
              "   18.559707641601562,\n",
              "   18.189069747924805,\n",
              "   18.197277069091797,\n",
              "   18.30978775024414,\n",
              "   18.34748649597168,\n",
              "   18.294713973999023,\n",
              "   18.375200271606445,\n",
              "   18.508651733398438,\n",
              "   18.445096969604492,\n",
              "   18.60246467590332,\n",
              "   18.53868865966797,\n",
              "   18.99312400817871,\n",
              "   18.659748077392578,\n",
              "   18.644628524780273,\n",
              "   19.0399227142334,\n",
              "   19.147165298461914,\n",
              "   19.07779884338379,\n",
              "   19.052793502807617,\n",
              "   19.374671936035156,\n",
              "   19.064651489257812,\n",
              "   19.588937759399414,\n",
              "   19.369264602661133,\n",
              "   19.438352584838867,\n",
              "   19.70136070251465,\n",
              "   19.596940994262695,\n",
              "   19.530290603637695,\n",
              "   19.761184692382812,\n",
              "   19.423938751220703,\n",
              "   19.514368057250977,\n",
              "   20.48798179626465,\n",
              "   19.411577224731445,\n",
              "   19.35904312133789,\n",
              "   19.538583755493164,\n",
              "   20.06546401977539,\n",
              "   19.633535385131836,\n",
              "   19.570968627929688,\n",
              "   19.878440856933594]},\n",
              " 2: {'loss': [878.9039306640625,\n",
              "   607.62646484375,\n",
              "   526.2774658203125,\n",
              "   473.35150146484375,\n",
              "   415.4793395996094,\n",
              "   386.4293212890625,\n",
              "   371.1916809082031,\n",
              "   361.8954772949219,\n",
              "   356.7703857421875,\n",
              "   354.8367919921875,\n",
              "   351.59735107421875,\n",
              "   344.6112365722656,\n",
              "   343.29302978515625,\n",
              "   341.7437744140625,\n",
              "   341.08331298828125,\n",
              "   341.5738830566406,\n",
              "   337.0918273925781,\n",
              "   337.20068359375,\n",
              "   333.7486572265625,\n",
              "   334.4925537109375,\n",
              "   332.3944091796875,\n",
              "   330.80120849609375,\n",
              "   328.2804870605469,\n",
              "   326.5567321777344,\n",
              "   326.67529296875,\n",
              "   324.7720031738281,\n",
              "   320.5627136230469,\n",
              "   319.4771728515625,\n",
              "   314.51568603515625,\n",
              "   312.1236572265625,\n",
              "   305.324462890625,\n",
              "   301.2455749511719,\n",
              "   295.0287780761719,\n",
              "   290.02886962890625,\n",
              "   282.8972473144531,\n",
              "   282.94384765625,\n",
              "   276.81268310546875,\n",
              "   270.6859436035156,\n",
              "   268.10888671875,\n",
              "   265.1949462890625,\n",
              "   256.5616760253906,\n",
              "   253.1292266845703,\n",
              "   246.96282958984375,\n",
              "   243.29248046875,\n",
              "   241.17855834960938,\n",
              "   240.3383331298828,\n",
              "   234.60952758789062,\n",
              "   225.47708129882812,\n",
              "   227.72145080566406,\n",
              "   225.7924041748047,\n",
              "   221.1560516357422,\n",
              "   214.2920379638672,\n",
              "   218.77000427246094,\n",
              "   213.93128967285156,\n",
              "   215.62240600585938,\n",
              "   207.96060180664062,\n",
              "   206.42430114746094,\n",
              "   199.35105895996094,\n",
              "   202.73472595214844,\n",
              "   201.5230255126953],\n",
              "  'root_mean_squared_error': [26.522249221801758,\n",
              "   22.50168228149414,\n",
              "   21.40087890625,\n",
              "   20.67943572998047,\n",
              "   19.573848724365234,\n",
              "   18.98106575012207,\n",
              "   18.677261352539062,\n",
              "   18.487895965576172,\n",
              "   18.38858413696289,\n",
              "   18.377470016479492,\n",
              "   18.31704330444336,\n",
              "   18.153106689453125,\n",
              "   18.14740562438965,\n",
              "   18.124114990234375,\n",
              "   18.116456985473633,\n",
              "   18.14716911315918,\n",
              "   18.031946182250977,\n",
              "   18.048572540283203,\n",
              "   17.956871032714844,\n",
              "   17.980810165405273,\n",
              "   17.9260311126709,\n",
              "   17.89215850830078,\n",
              "   17.82231903076172,\n",
              "   17.781112670898438,\n",
              "   17.776866912841797,\n",
              "   17.722736358642578,\n",
              "   17.609268188476562,\n",
              "   17.570425033569336,\n",
              "   17.42876625061035,\n",
              "   17.361833572387695,\n",
              "   17.159337997436523,\n",
              "   17.040287017822266,\n",
              "   16.859811782836914,\n",
              "   16.706186294555664,\n",
              "   16.48924446105957,\n",
              "   16.490264892578125,\n",
              "   16.297197341918945,\n",
              "   16.1154727935791,\n",
              "   16.032121658325195,\n",
              "   15.938791275024414,\n",
              "   15.667459487915039,\n",
              "   15.554062843322754,\n",
              "   15.351627349853516,\n",
              "   15.233268737792969,\n",
              "   15.160304069519043,\n",
              "   15.133721351623535,\n",
              "   14.93950366973877,\n",
              "   14.634246826171875,\n",
              "   14.710369110107422,\n",
              "   14.646770477294922,\n",
              "   14.484237670898438,\n",
              "   14.244744300842285,\n",
              "   14.404997825622559,\n",
              "   14.231119155883789,\n",
              "   14.294047355651855,\n",
              "   14.018065452575684,\n",
              "   13.967950820922852,\n",
              "   13.712011337280273,\n",
              "   13.831043243408203,\n",
              "   13.787425994873047],\n",
              "  'val_loss': [611.2109985351562,\n",
              "   563.2152709960938,\n",
              "   473.3708801269531,\n",
              "   396.0224304199219,\n",
              "   382.30364990234375,\n",
              "   343.7655944824219,\n",
              "   335.96826171875,\n",
              "   336.9365234375,\n",
              "   333.12518310546875,\n",
              "   331.1003112792969,\n",
              "   330.1914367675781,\n",
              "   329.16461181640625,\n",
              "   329.5161437988281,\n",
              "   327.5645751953125,\n",
              "   327.7292175292969,\n",
              "   327.4254150390625,\n",
              "   327.5616760253906,\n",
              "   327.1263122558594,\n",
              "   326.70037841796875,\n",
              "   326.2820739746094,\n",
              "   327.2370910644531,\n",
              "   327.5900573730469,\n",
              "   326.86724853515625,\n",
              "   332.01580810546875,\n",
              "   328.1720275878906,\n",
              "   328.97314453125,\n",
              "   334.4307861328125,\n",
              "   336.3591003417969,\n",
              "   339.2206726074219,\n",
              "   342.86431884765625,\n",
              "   339.92510986328125,\n",
              "   339.97479248046875,\n",
              "   374.9920654296875,\n",
              "   354.17596435546875,\n",
              "   350.09637451171875,\n",
              "   383.1626892089844,\n",
              "   373.2864990234375,\n",
              "   368.41412353515625,\n",
              "   360.0668029785156,\n",
              "   364.39215087890625,\n",
              "   376.618896484375,\n",
              "   393.013671875,\n",
              "   427.6201477050781,\n",
              "   361.8675842285156,\n",
              "   386.144775390625,\n",
              "   386.7800598144531,\n",
              "   369.2324523925781,\n",
              "   371.2470397949219,\n",
              "   372.8243713378906,\n",
              "   369.0478820800781,\n",
              "   380.4371643066406,\n",
              "   376.62286376953125,\n",
              "   389.6343078613281,\n",
              "   430.26434326171875,\n",
              "   391.8576354980469,\n",
              "   407.6078186035156,\n",
              "   381.7658996582031,\n",
              "   407.826904296875,\n",
              "   381.5157775878906,\n",
              "   423.0013122558594],\n",
              "  'val_root_mean_squared_error': [23.566402435302734,\n",
              "   22.97296905517578,\n",
              "   21.03223991394043,\n",
              "   19.24835777282715,\n",
              "   18.99596405029297,\n",
              "   17.95553207397461,\n",
              "   17.774518966674805,\n",
              "   17.836549758911133,\n",
              "   17.780773162841797,\n",
              "   17.745637893676758,\n",
              "   17.749897003173828,\n",
              "   17.74652862548828,\n",
              "   17.772674560546875,\n",
              "   17.744503021240234,\n",
              "   17.74757194519043,\n",
              "   17.756879806518555,\n",
              "   17.786046981811523,\n",
              "   17.767868041992188,\n",
              "   17.767044067382812,\n",
              "   17.759357452392578,\n",
              "   17.7910213470459,\n",
              "   17.804611206054688,\n",
              "   17.79401397705078,\n",
              "   17.932151794433594,\n",
              "   17.82486915588379,\n",
              "   17.85755729675293,\n",
              "   18.005809783935547,\n",
              "   18.061254501342773,\n",
              "   18.101842880249023,\n",
              "   18.2313232421875,\n",
              "   18.159486770629883,\n",
              "   18.15569496154785,\n",
              "   19.088163375854492,\n",
              "   18.54316520690918,\n",
              "   18.423917770385742,\n",
              "   19.293508529663086,\n",
              "   19.045103073120117,\n",
              "   18.926433563232422,\n",
              "   18.702054977416992,\n",
              "   18.816978454589844,\n",
              "   19.133636474609375,\n",
              "   19.547134399414062,\n",
              "   20.40648651123047,\n",
              "   18.74445152282715,\n",
              "   19.384340286254883,\n",
              "   19.397323608398438,\n",
              "   18.94614028930664,\n",
              "   18.999711990356445,\n",
              "   19.044471740722656,\n",
              "   18.94122314453125,\n",
              "   19.23628044128418,\n",
              "   19.140167236328125,\n",
              "   19.47169303894043,\n",
              "   20.47974967956543,\n",
              "   19.525684356689453,\n",
              "   19.9234561920166,\n",
              "   19.27754020690918,\n",
              "   19.935365676879883,\n",
              "   19.269615173339844,\n",
              "   20.306167602539062]},\n",
              " 3: {'loss': [938.735107421875,\n",
              "   609.7942504882812,\n",
              "   538.7774047851562,\n",
              "   479.5564270019531,\n",
              "   418.34539794921875,\n",
              "   379.86663818359375,\n",
              "   372.68798828125,\n",
              "   365.4781494140625,\n",
              "   357.2276916503906,\n",
              "   352.5317687988281,\n",
              "   352.0689392089844,\n",
              "   348.3274230957031,\n",
              "   345.7362365722656,\n",
              "   344.9025573730469,\n",
              "   344.87933349609375,\n",
              "   340.0618896484375,\n",
              "   337.9820556640625,\n",
              "   338.05718994140625,\n",
              "   334.5101623535156,\n",
              "   335.4886474609375,\n",
              "   332.75958251953125,\n",
              "   333.8918762207031,\n",
              "   331.7403564453125,\n",
              "   330.4725036621094,\n",
              "   330.80419921875,\n",
              "   328.2612609863281,\n",
              "   327.5483093261719,\n",
              "   325.1216735839844,\n",
              "   321.9036560058594,\n",
              "   319.2026062011719,\n",
              "   316.49005126953125,\n",
              "   312.1598815917969,\n",
              "   312.1509094238281,\n",
              "   307.3312072753906,\n",
              "   302.8945617675781,\n",
              "   298.15374755859375,\n",
              "   293.62530517578125,\n",
              "   290.5069885253906,\n",
              "   287.4031066894531,\n",
              "   282.82855224609375,\n",
              "   274.7946472167969,\n",
              "   271.3631591796875,\n",
              "   270.5563659667969,\n",
              "   263.6015625,\n",
              "   261.0653381347656,\n",
              "   256.1232604980469,\n",
              "   249.90501403808594,\n",
              "   248.64129638671875,\n",
              "   243.4510040283203,\n",
              "   241.98764038085938,\n",
              "   234.99649047851562,\n",
              "   234.5386962890625,\n",
              "   228.23757934570312,\n",
              "   233.03285217285156,\n",
              "   225.4777069091797,\n",
              "   225.80360412597656,\n",
              "   218.85089111328125,\n",
              "   219.75416564941406,\n",
              "   209.58816528320312,\n",
              "   212.0750274658203],\n",
              "  'root_mean_squared_error': [27.789024353027344,\n",
              "   22.512969970703125,\n",
              "   21.66411781311035,\n",
              "   20.741552352905273,\n",
              "   19.59735679626465,\n",
              "   18.78192138671875,\n",
              "   18.69159698486328,\n",
              "   18.559711456298828,\n",
              "   18.380056381225586,\n",
              "   18.292957305908203,\n",
              "   18.31616973876953,\n",
              "   18.23431396484375,\n",
              "   18.189363479614258,\n",
              "   18.184751510620117,\n",
              "   18.19919204711914,\n",
              "   18.08653450012207,\n",
              "   18.043079376220703,\n",
              "   18.056875228881836,\n",
              "   17.967147827148438,\n",
              "   18.00402069091797,\n",
              "   17.933324813842773,\n",
              "   17.974740982055664,\n",
              "   17.919391632080078,\n",
              "   17.88995361328125,\n",
              "   17.90493392944336,\n",
              "   17.83513832092285,\n",
              "   17.819791793823242,\n",
              "   17.752925872802734,\n",
              "   17.664880752563477,\n",
              "   17.5906925201416,\n",
              "   17.51728630065918,\n",
              "   17.38961410522461,\n",
              "   17.387531280517578,\n",
              "   17.244821548461914,\n",
              "   17.111722946166992,\n",
              "   16.970516204833984,\n",
              "   16.836471557617188,\n",
              "   16.736976623535156,\n",
              "   16.646310806274414,\n",
              "   16.502756118774414,\n",
              "   16.25305938720703,\n",
              "   16.14285659790039,\n",
              "   16.11750602722168,\n",
              "   15.896958351135254,\n",
              "   15.814613342285156,\n",
              "   15.655560493469238,\n",
              "   15.45276927947998,\n",
              "   15.409538269042969,\n",
              "   15.230566024780273,\n",
              "   15.190467834472656,\n",
              "   14.950207710266113,\n",
              "   14.936681747436523,\n",
              "   14.726570129394531,\n",
              "   14.884113311767578,\n",
              "   14.626952171325684,\n",
              "   14.634818077087402,\n",
              "   14.395820617675781,\n",
              "   14.424784660339355,\n",
              "   14.062868118286133,\n",
              "   14.15147876739502],\n",
              "  'val_loss': [543.5070190429688,\n",
              "   515.7391357421875,\n",
              "   446.30328369140625,\n",
              "   396.2662658691406,\n",
              "   371.1374206542969,\n",
              "   349.88555908203125,\n",
              "   345.21331787109375,\n",
              "   344.892333984375,\n",
              "   343.5648193359375,\n",
              "   338.11944580078125,\n",
              "   338.0902404785156,\n",
              "   336.3604736328125,\n",
              "   335.6632385253906,\n",
              "   335.39739990234375,\n",
              "   337.6930236816406,\n",
              "   333.5752868652344,\n",
              "   332.90521240234375,\n",
              "   342.4808349609375,\n",
              "   332.2398986816406,\n",
              "   337.7545166015625,\n",
              "   347.433349609375,\n",
              "   331.30865478515625,\n",
              "   333.0228271484375,\n",
              "   330.6983337402344,\n",
              "   329.680419921875,\n",
              "   371.1142578125,\n",
              "   376.04180908203125,\n",
              "   331.7418212890625,\n",
              "   329.59130859375,\n",
              "   362.996826171875,\n",
              "   333.66302490234375,\n",
              "   334.2510681152344,\n",
              "   334.042236328125,\n",
              "   341.0955505371094,\n",
              "   334.36212158203125,\n",
              "   340.7421875,\n",
              "   344.7980651855469,\n",
              "   339.60430908203125,\n",
              "   345.58111572265625,\n",
              "   348.14892578125,\n",
              "   374.2842712402344,\n",
              "   351.4376220703125,\n",
              "   359.47613525390625,\n",
              "   362.0862121582031,\n",
              "   374.5946960449219,\n",
              "   361.61041259765625,\n",
              "   367.45831298828125,\n",
              "   387.42449951171875,\n",
              "   372.3108215332031,\n",
              "   380.510986328125,\n",
              "   374.3191223144531,\n",
              "   381.4232482910156,\n",
              "   372.75543212890625,\n",
              "   372.9064025878906,\n",
              "   375.0494689941406,\n",
              "   369.3274230957031,\n",
              "   371.1288757324219,\n",
              "   368.83233642578125,\n",
              "   385.232177734375,\n",
              "   392.61309814453125],\n",
              "  'val_root_mean_squared_error': [21.21415901184082,\n",
              "   21.544906616210938,\n",
              "   20.019855499267578,\n",
              "   19.06199836730957,\n",
              "   18.612770080566406,\n",
              "   18.067298889160156,\n",
              "   18.000934600830078,\n",
              "   18.052797317504883,\n",
              "   18.018659591674805,\n",
              "   17.92544174194336,\n",
              "   17.957317352294922,\n",
              "   17.92853546142578,\n",
              "   17.919546127319336,\n",
              "   17.94525146484375,\n",
              "   18.027563095092773,\n",
              "   17.922069549560547,\n",
              "   17.914094924926758,\n",
              "   18.153688430786133,\n",
              "   17.91431999206543,\n",
              "   18.021760940551758,\n",
              "   18.31641960144043,\n",
              "   17.914426803588867,\n",
              "   17.965805053710938,\n",
              "   17.899721145629883,\n",
              "   17.87294578552246,\n",
              "   18.970287322998047,\n",
              "   19.098007202148438,\n",
              "   17.950578689575195,\n",
              "   17.887887954711914,\n",
              "   18.778566360473633,\n",
              "   17.999977111816406,\n",
              "   18.01853370666504,\n",
              "   18.015390396118164,\n",
              "   18.19974708557129,\n",
              "   18.01851463317871,\n",
              "   18.192081451416016,\n",
              "   18.29517364501953,\n",
              "   18.16598892211914,\n",
              "   18.313932418823242,\n",
              "   18.395936965942383,\n",
              "   19.06137466430664,\n",
              "   18.475862503051758,\n",
              "   18.691974639892578,\n",
              "   18.759904861450195,\n",
              "   19.073265075683594,\n",
              "   18.738550186157227,\n",
              "   18.891279220581055,\n",
              "   19.408580780029297,\n",
              "   19.020854949951172,\n",
              "   19.234460830688477,\n",
              "   19.077470779418945,\n",
              "   19.250532150268555,\n",
              "   19.035491943359375,\n",
              "   19.03326416015625,\n",
              "   19.0925350189209,\n",
              "   18.938034057617188,\n",
              "   18.982532501220703,\n",
              "   18.924264907836914,\n",
              "   19.3485050201416,\n",
              "   19.539112091064453]},\n",
              " 4: {'loss': [843.4343872070312,\n",
              "   614.3682250976562,\n",
              "   528.9163208007812,\n",
              "   461.8777770996094,\n",
              "   415.20892333984375,\n",
              "   393.07000732421875,\n",
              "   374.8234558105469,\n",
              "   365.2817077636719,\n",
              "   359.8932189941406,\n",
              "   356.919677734375,\n",
              "   352.67291259765625,\n",
              "   349.601806640625,\n",
              "   344.3331298828125,\n",
              "   342.778076171875,\n",
              "   341.03375244140625,\n",
              "   340.8487854003906,\n",
              "   338.88641357421875,\n",
              "   336.9070129394531,\n",
              "   336.59515380859375,\n",
              "   333.4661865234375,\n",
              "   334.3486022949219,\n",
              "   333.2825927734375,\n",
              "   331.4017639160156,\n",
              "   330.93853759765625,\n",
              "   330.40264892578125,\n",
              "   328.5008544921875,\n",
              "   327.78973388671875,\n",
              "   326.31201171875,\n",
              "   325.2901916503906,\n",
              "   321.5288391113281,\n",
              "   318.9732971191406,\n",
              "   317.8075866699219,\n",
              "   315.8164367675781,\n",
              "   310.4502868652344,\n",
              "   308.2381591796875,\n",
              "   299.5613098144531,\n",
              "   297.803955078125,\n",
              "   293.1122741699219,\n",
              "   281.82354736328125,\n",
              "   282.1811828613281,\n",
              "   276.8218688964844,\n",
              "   273.328857421875,\n",
              "   266.5565490722656,\n",
              "   257.41015625,\n",
              "   259.3002624511719,\n",
              "   256.89697265625,\n",
              "   250.63771057128906,\n",
              "   244.7698516845703,\n",
              "   244.68258666992188,\n",
              "   238.14425659179688,\n",
              "   236.33009338378906,\n",
              "   228.94772338867188,\n",
              "   224.25230407714844,\n",
              "   221.3860626220703,\n",
              "   223.15496826171875,\n",
              "   218.72018432617188,\n",
              "   214.09889221191406,\n",
              "   208.3524932861328,\n",
              "   205.3004150390625,\n",
              "   205.5502471923828],\n",
              "  'root_mean_squared_error': [25.792001724243164,\n",
              "   22.57549285888672,\n",
              "   21.44053840637207,\n",
              "   20.394285202026367,\n",
              "   19.488245010375977,\n",
              "   19.102920532226562,\n",
              "   18.722169876098633,\n",
              "   18.544614791870117,\n",
              "   18.445148468017578,\n",
              "   18.397308349609375,\n",
              "   18.315662384033203,\n",
              "   18.266376495361328,\n",
              "   18.15457534790039,\n",
              "   18.13560676574707,\n",
              "   18.104881286621094,\n",
              "   18.116703033447266,\n",
              "   18.078767776489258,\n",
              "   18.03616714477539,\n",
              "   18.040834426879883,\n",
              "   17.95720672607422,\n",
              "   17.994905471801758,\n",
              "   17.970062255859375,\n",
              "   17.922889709472656,\n",
              "   17.91292953491211,\n",
              "   17.900184631347656,\n",
              "   17.85067367553711,\n",
              "   17.829980850219727,\n",
              "   17.795528411865234,\n",
              "   17.764129638671875,\n",
              "   17.657268524169922,\n",
              "   17.583463668823242,\n",
              "   17.550369262695312,\n",
              "   17.486480712890625,\n",
              "   17.32841682434082,\n",
              "   17.26609992980957,\n",
              "   17.013986587524414,\n",
              "   16.962718963623047,\n",
              "   16.817962646484375,\n",
              "   16.480562210083008,\n",
              "   16.494091033935547,\n",
              "   16.32916259765625,\n",
              "   16.21113395690918,\n",
              "   16.00213623046875,\n",
              "   15.711174964904785,\n",
              "   15.768131256103516,\n",
              "   15.69215202331543,\n",
              "   15.491218566894531,\n",
              "   15.303423881530762,\n",
              "   15.299040794372559,\n",
              "   15.081354141235352,\n",
              "   15.019856452941895,\n",
              "   14.77102279663086,\n",
              "   14.60903549194336,\n",
              "   14.507427215576172,\n",
              "   14.571295738220215,\n",
              "   14.41219711303711,\n",
              "   14.250700950622559,\n",
              "   14.044840812683105,\n",
              "   13.933723449707031,\n",
              "   13.942644119262695],\n",
              "  'val_loss': [740.5540771484375,\n",
              "   557.3971557617188,\n",
              "   457.0245361328125,\n",
              "   397.286865234375,\n",
              "   354.6658020019531,\n",
              "   356.11773681640625,\n",
              "   349.2528076171875,\n",
              "   345.5325927734375,\n",
              "   345.4644470214844,\n",
              "   344.4283752441406,\n",
              "   340.23406982421875,\n",
              "   339.48309326171875,\n",
              "   337.07049560546875,\n",
              "   339.36767578125,\n",
              "   339.0896911621094,\n",
              "   336.775146484375,\n",
              "   335.43231201171875,\n",
              "   335.9508361816406,\n",
              "   333.629150390625,\n",
              "   334.008544921875,\n",
              "   333.7992248535156,\n",
              "   332.90533447265625,\n",
              "   339.1260986328125,\n",
              "   341.8422546386719,\n",
              "   335.8684997558594,\n",
              "   332.708251953125,\n",
              "   337.8701477050781,\n",
              "   337.72906494140625,\n",
              "   334.1365966796875,\n",
              "   334.5313415527344,\n",
              "   339.15576171875,\n",
              "   336.9246826171875,\n",
              "   342.22149658203125,\n",
              "   349.48858642578125,\n",
              "   343.9940490722656,\n",
              "   343.58056640625,\n",
              "   355.6880187988281,\n",
              "   342.15887451171875,\n",
              "   357.24285888671875,\n",
              "   348.6787109375,\n",
              "   358.0255126953125,\n",
              "   347.41021728515625,\n",
              "   355.7752380371094,\n",
              "   370.1014404296875,\n",
              "   355.5964050292969,\n",
              "   354.7295227050781,\n",
              "   363.9292907714844,\n",
              "   369.77264404296875,\n",
              "   364.2337646484375,\n",
              "   376.8946228027344,\n",
              "   368.07666015625,\n",
              "   375.3832092285156,\n",
              "   392.16259765625,\n",
              "   372.0903625488281,\n",
              "   382.3800964355469,\n",
              "   378.69140625,\n",
              "   388.4037170410156,\n",
              "   373.7123718261719,\n",
              "   380.4812316894531,\n",
              "   374.6261291503906],\n",
              "  'val_root_mean_squared_error': [26.28573989868164,\n",
              "   22.723072052001953,\n",
              "   20.58603286743164,\n",
              "   19.158544540405273,\n",
              "   18.018333435058594,\n",
              "   18.231746673583984,\n",
              "   18.088687896728516,\n",
              "   18.0435733795166,\n",
              "   18.106094360351562,\n",
              "   18.066041946411133,\n",
              "   18.005903244018555,\n",
              "   18.018239974975586,\n",
              "   17.970870971679688,\n",
              "   18.0657958984375,\n",
              "   18.068458557128906,\n",
              "   18.02740478515625,\n",
              "   17.998811721801758,\n",
              "   18.027318954467773,\n",
              "   17.957439422607422,\n",
              "   17.973445892333984,\n",
              "   17.988651275634766,\n",
              "   17.96269416809082,\n",
              "   18.131032943725586,\n",
              "   18.192609786987305,\n",
              "   18.05687713623047,\n",
              "   17.97565460205078,\n",
              "   18.115991592407227,\n",
              "   18.1228084564209,\n",
              "   18.020931243896484,\n",
              "   18.02779197692871,\n",
              "   18.149681091308594,\n",
              "   18.093509674072266,\n",
              "   18.234893798828125,\n",
              "   18.423696517944336,\n",
              "   18.28338623046875,\n",
              "   18.271560668945312,\n",
              "   18.585994720458984,\n",
              "   18.23666763305664,\n",
              "   18.635149002075195,\n",
              "   18.410856246948242,\n",
              "   18.661701202392578,\n",
              "   18.37434196472168,\n",
              "   18.602224349975586,\n",
              "   18.975400924682617,\n",
              "   18.595205307006836,\n",
              "   18.565284729003906,\n",
              "   18.818166732788086,\n",
              "   18.977155685424805,\n",
              "   18.82655906677246,\n",
              "   19.15496253967285,\n",
              "   18.929277420043945,\n",
              "   19.120685577392578,\n",
              "   19.548608779907227,\n",
              "   19.03131675720215,\n",
              "   19.297473907470703,\n",
              "   19.20277214050293,\n",
              "   19.460721969604492,\n",
              "   19.070039749145508,\n",
              "   19.24277114868164,\n",
              "   19.092323303222656]}}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "209.0834991455078\n",
            "14.052566528320312\n",
            "382.9966186523437\n",
            "19.2991024017334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9-K3Mgs8weVG",
      "metadata": {
        "id": "9-K3Mgs8weVG"
      },
      "outputs": [],
      "source": [
        "# Create overview of layers in model\n",
        "# tf.keras.utils.plot_model(concat_model, to_file='model.png', show_shapes=True, show_layer_names=True, rankdir='TB')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = np.array(np.zeros(2))\n",
        "train_val = np.array(np.zeros(2))\n",
        "test_loss = np.array(np.zeros(2))\n",
        "test_val = np.array(np.zeros(2))\n",
        "\n",
        "for epoch in loss_df.items():\n",
        "  train_loss += epoch[1]['loss']\n",
        "  train_val += epoch[1]['root_mean_squared_error']\n",
        "  test_loss += epoch[1]['val_loss']\n",
        "  test_val += epoch[1]['val_root_mean_squared_error']\n",
        "\n",
        "display(test_val)"
      ],
      "metadata": {
        "id": "DCDlt7aXKX16"
      },
      "id": "DCDlt7aXKX16",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UiMunH5vM8wU"
      },
      "id": "UiMunH5vM8wU",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "K_fold_grafiekje_data_aug_zonder_outliers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}