{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4940683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_images(data, images, names):\n",
    "    \"\"\"\n",
    "    This function merges the images to the corresponding row of the dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    data['Image'] = \"\"\n",
    "    \n",
    "    # loop over all the names of the images\n",
    "    for index_image, name_image in enumerate(names):\n",
    "        \n",
    "        # loop over all the names in the dataframe\n",
    "        for index_csv, name_csv in enumerate(data['Id']):\n",
    "            \n",
    "            # if the names match, add the image to the row of the dataframe\n",
    "            if name_image == name_csv:\n",
    "                data['Image'][index_csv] = images[index_image]\n",
    "\n",
    "    return data\n",
    "\n",
    "merged_train_data = merge_images(csv_train_data, train_imgs, train_names)\n",
    "merged_test_data = merge_images(csv_test_data, test_imgs, test_names)\n",
    "display(merged_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e71973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tabular data from github\n",
    "csv_train_data = pd.read_csv('https://raw.githubusercontent.com/ilsefeenstra/Fitgirls0011/main/petfinder-pawpularity-score/train.csv')\n",
    "csv_test_data = pd.read_csv('https://raw.githubusercontent.com/ilsefeenstra/Fitgirls0011/main/petfinder-pawpularity-score/test.csv')\n",
    "sample_submission = pd.read_csv('https://raw.githubusercontent.com/ilsefeenstra/Fitgirls0011/main/petfinder-pawpularity-score/sample_submission.csv')\n",
    "sample_submission.head()\n",
    "csv_train_data.head()\n",
    "\n",
    "# no missing values present, so no samples dropped\n",
    "csv_train_data.dropna()\n",
    "\n",
    "# scale the pawpularity score to numbers between 0 and 1\n",
    "csv_train_data['Pawpularity'] = csv_train_data['Pawpularity'] / 100\n",
    "sample_submission['Pawpularity'] = sample_submission['Pawpularity'] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cba24f90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tabular_NN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_5032/514622801.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Compile model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtabular_NN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMeanSquaredError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Train our model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tabular_NN' is not defined"
     ]
    }
   ],
   "source": [
    "# Part of code from: https://www.tensorflow.org/tutorials/keras/regression\n",
    "\n",
    "# Compile model\n",
    "tabular_NN.compile(loss=MeanSquaredError())\n",
    "\n",
    "# Train our model\n",
    "history = tabular_NN.fit(x_train_tabular, y_train, batch_size=20, epochs=20, validation_split=.1)\n",
    "\n",
    "# Store loss during training in DataFrame\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()\n",
    "\n",
    "def plot_loss(history):\n",
    "    \"\"\"\n",
    "    Plot loss during epochs of training a neural network.\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plot_loss(history)\n",
    "\n",
    "# See how the model scored\n",
    "loss = tabular_NN.evaluate(x_test_tabular, y_test)\n",
    "\n",
    "# Print to 3 decimals\n",
    "print(f'Test loss: {loss:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57330fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_x, train_y, val_x, val_y, preprocess={}, epochs=20, augment={}):\n",
    "    \"\"\"\n",
    "    Take a model, training data (x,y), validation data (x,y), preprocessing \n",
    "    methods, number of epochs and data augmentation methods as input. \n",
    "    Train and evaluate the model.\n",
    "    This function is taken from CNN_cifar assignment.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use data preprocessing and augmentation methods on training data and\n",
    "    # fit the data\n",
    "    train_gen = preprocessing.image.ImageDataGenerator(**preprocess, **augment)\n",
    "    train_gen.fit(train_x) \n",
    "\n",
    "    # Use data preprocessing and augmentation methods on testing data and\n",
    "    # fit the data\n",
    "    val_gen = preprocessing.image.ImageDataGenerator(**preprocess)\n",
    "    val_gen.fit(train_x)\n",
    "    \n",
    "    # \n",
    "    history = model.fit(train_gen.flow(train_x, train_y), epochs=epochs, \n",
    "                        validation_data=val_gen.flow(val_x, val_y))\n",
    "\n",
    "    fig, axs = plt.subplots(1,2,figsize=(20,5)) \n",
    "\n",
    "    for i, metric in enumerate(['loss', tf.keras.metrics.RootMeanSquaredError()]):\n",
    "        axs[i].plot(history.history[metric])\n",
    "        axs[i].plot(history.history['val_'+metric])\n",
    "        axs[i].legend(['training', 'validation'], loc='best')\n",
    "\n",
    "        axs[i].set_title('Model '+metric)\n",
    "        axs[i].set_ylabel(metric)\n",
    "        axs[i].set_xlabel('epoch')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Validation Accuracy: {model.evaluate(val_gen.flow(val_x, val_y))[1]}\")\n",
    "\n",
    "# Create neural network for image data and get summary of model\n",
    "image_size = (64, 64, 3)\n",
    "image_NN = build_convol_net(image_size, 100)\n",
    "image_NN.summary()\n",
    "\n",
    "#train_and_evaluate(image_NN, train_imgs_array, y_train, test_imgs_array, y_test,\n",
    "                  #preprocess = {'featurewise_center': True, 'featurewise_std_normalization': True})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
